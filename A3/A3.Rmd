---
title: "Assignment 3 - Finding fast growing firms"
subtitle: "Course: ECBS6067 - Prediction with Machine Learning for Economists"
author: "Anja Hahn, Teresa Huebel & Anne Valder"
date: "2023-12-14"
output:
  pdf_document: default
  html_document:
  df_print: paged
---

```{r, include=FALSE}
# Clear memory
rm(list=ls())

# set global chunk options
knitr::opts_chunk$set(include = FALSE, fig.align = 'center', cache = TRUE,
                      warning = FALSE, message = FALSE, echo = FALSE)

options(scipen = 999)

```

```{r setup, include=FALSE}
# Set options for markdown
knitr::opts_knit$set(root.dir = 'C:/Users/AnjaHahn/OneDrive - DataScience Service GmbH/WU/Courses/CEU Prediction with Machine Learning for Economists/da_case_studies')

# store different directories here:
# C:/Users/thuebel/OneDrive - WU Wien/Docs/06 Courses and Study/02 Prediction with ML for Economists/da_case_studies
# C:/Users/avalder/OneDrive - WU Wien/Documents/Study/WS_23_24/Pred_MLE_Econ/da_case_studies
# C:/Users/AnjaHahn/OneDrive - DataScience Service GmbH/WU/Courses/CEU Prediction with Machine Learning for Economists/da_case_studies

```

<!-- Set directory, load functions and theme  --> 
```{r, include=FALSE}
#getwd()
source("set-data-directory.R")
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")


data_in <- paste(data_dir,"bisnode-firms","clean/", sep = "/")
#use_case_dir <- "ch17-predicting-firm-exit/"
```

```{r,include=FALSE, warning=FALSE}
#load libraries
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
library(modelsummary)
library(treeshap)
library(summarytools)
# add
```

<!-- Load data  --> 
```{r,include=FALSE}
bisnode <- read_csv(paste(data_in,"cs_bisnode_panel.csv", sep = "/")) # 287829 obs. of 48 variables 
```

<!-- Data summary  --> 
```{r, include=FALSE}
# count missing values
colSums(is.na(bisnode))

# look at data
bisnode %>% glimpse()

# look at character variables
bisnode %>%
  group_by(gender) %>%
  summarise(n = n(), mean = mean(sales))

# overall summary statistics
# descr(bisnode)

```

<!-- Data preparation  --> 

```{r}

# convert character variables to one-hot-encoding
bisnode <- bisnode %>% 
  mutate_if(is.character, as.factor)
dmy <- dummyVars("~ gender + origin + region_m", data = bisnode, fullRank = TRUE)

new_dat <- data.frame(predict(dmy, newdata = bisnode))
# @Anne: From your experience with A2, will the column names with "." make trouble now that we one-hot-encode in the beginning?

# add new variables to data and discard old
bisnode <- cbind(bisnode, new_dat)
bisnode <- bisnode %>%
  select(-c("gender", "origin", "region_m"))



```

<!-- Sample design  --> 

```{r, include=FALSE}
####################################
# Sample design
####################################
# collect "weird stuff" here:
# Company with ID 12212152320 had a massive explosion from 2012 to 2013 (0 to 63 employees etc) - how to treat?



# add all missing year and company combinations
bisnode <- bisnode %>%
  complete(year, comp_id)

# drop variables with many NAs
bisnode <- bisnode %>%
  select(-c(COGS, finished_prod, net_dom_sales, net_exp_sales, wages, D))
# potentially also drop unused years (e.g. 2016)

# find the years with the least missing values
bisnode %>%
  group_by(year) %>%
  summarise(missing = sum(is.na(sales))) %>%
  arrange(missing)
# biggest sample would be 2013-2014

# generate status_alive, we only want to analyse active firms
bisnode  <- bisnode %>%
  mutate(status_alive = sales > 0 & !is.na(sales) %>%
           as.numeric(.))

# generate different variations to display sales
bisnode <- bisnode %>%
  filter(status_alive == TRUE) %>% # only look at firms that have some sales
  mutate(ln_sales = log(sales), # if we want to include firms not_alive need to specially treat 0
         sales_mil=sales/1000000,
         sales_mil_log = ifelse(sales > 0, log(sales_mil), 0))
# this already windsorizes sales

# look at cross section
bisnode <- bisnode %>%
  filter(status_alive == TRUE) %>% # only look at firms that have some sales
  # look at firms below 10m euro revenues and above 1000 euros
  filter(!(sales_mil > 10)) %>%
  filter(!(sales_mil < 0.001))

# generate different variations to display labor_avg
bisnode <- bisnode %>%
  mutate(ln_labor_avg = ifelse(labor_avg > 0, log(labor_avg), 0))
```


<!-- Data visualization  --> 

```{r, include=FALSE}
# visualize sales
plot_sales <-  ggplot(bisnode, aes(x = sales)) +
  geom_histogram(bins = 100, fill = color[1], color = color.outline, alpha = 0.8) +
  scale_x_log10() +
  labs(x = "Sales (log)", y = "Frequency")
plot_sales

# visualize labor_avg
plot_labor_avg <- bisnode %>% filter(labor_avg > 0) %>%
  ggplot(aes(x = labor_avg)) +
  geom_histogram(bins = 100, fill = color[1], color = color.outline, alpha = 0.8) +
  labs(x = "Average Number of Employees", y = "Frequency") +
  xlim(0, 5)
plot_labor_avg
# in codebook: N/12 --- (annual average / 12, it's not ideal, sorry) 
# what does this mean?


```



<!-- Label engineering  --> 

```{r, include=FALSE}
####################################
# Label engineering
####################################


###################################
# Creation of different labels: sales
###################################

# YoY relative sales growth
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(sales_growth = (sales/lag(sales) -1)) %>%
  ungroup()

# YoY sales growth as logdiff
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(d1_sales_mil_log = sales_mil_log - Lag(sales_mil_log) ) %>%
  ungroup()


###################################
# Visualization sales growth

# make histogram that shows both sales growth and d1_sales_mil_log
plot_sales_growth_both <- ggplot(bisnode, aes(x = sales_growth)) +
  geom_histogram(aes(fill = "Percentage Change"), bins = 100, color = color.outline, alpha = 0.5) +
  geom_histogram(aes(x = d1_sales_mil_log, fill = "Log-Differences"), bins = 100, 
                 color = color.outline, alpha = 0.5) +
  scale_x_log10() +
  labs(x = "Sales growth (log)", y = "Frequency") +
  scale_fill_manual(values = c("Percentage Change" = color[1], "Log-Differences" = color[2]), 
                    name = "Growth Measures")
plot_sales_growth_both
# It makes a difference whether relative growth is measured in log(diff) or percentage change
# since logdiff is an inaccurate approximation for high changes this is especially relevant for high growth firms


###################################
# Creation of different labels: employment
###################################
# YoY relative employee growth
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(labor_growth = (labor_avg/lag(labor_avg) - 1)) %>%
  ungroup()

# YoY employee growth as logdiff
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(d1_labor_avg_log = ln_labor_avg - Lag(ln_labor_avg) ) %>%
  ungroup()

# birch index (weighted index of absolute and relative employment growth)
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(birch_ind = ((labor_avg - lag(labor_avg)) * (labor_avg/lag(labor_avg) - 1)) ) %>%
  ungroup()

###################################
# Visualization employment growth

#make histogram that shows both employee growth and d1_labor_avg_log
plot_labor_growth_both <- ggplot(bisnode, aes(x = labor_growth)) +
  geom_histogram(aes(fill = "Percentage Change"), bins = 100, color = color.outline, alpha = 0.5) +
  geom_histogram(aes(x = d1_labor_avg_log, fill = "Log-Differences"), bins = 100, 
                 color = color.outline, alpha = 0.5) +
  labs(x = "Employee growth (log)", y = "Frequency") +
  scale_fill_manual(values = c("Percentage Change" = color[1], "Log-Differences" = color[2]), 
                    name = "Growth Measures") +
  scale_x_log10()
plot_labor_growth_both

####################################
# notes on growth
####################################
# different types of growth
# yoy relative growth in sales
# average yoy relative growth in sales over 2 years
# yoy relative growth in employees
# average yoy relative growth in employees over 2 years

# cutoff:
# classify as "fast" if they meet a combination of the above?
# OECD definition: 20% average growth in sales or employees over 3 years
# do the same but only with 2 years?
# in literature it has been suggested to use fastest growing percintile 8e.g. to 5% of firms)
# see: https://www.diva-portal.org/smash/get/diva2:605658/FULLTEXT01.pdf
# rather than hard cutoff --> but not meaningful for us if we want to compare growth in different industries
# a lot of firms with 0-1 employees: might need a lower absolute threshold for growth such that tiny firms that
# add one extra employee are not classified as fast growing?

```

<!-- Feature engineering  --> 

```{r, include=FALSE}
# missing values
# interactions (irrelevant ML?)
# functional form (irrelevant ML?)
# select and store variables for each model (paste formulas)
```

<!-- Preparation for prediction & CV  --> 
```{r,include=FALSE}

set.seed(123456)

work_indices <- as.integer(createDataPartition(bisnode$default, p = 0.8, list = FALSE))
bisnode_work <- bisnode[train_indices, ]
bisnode_holdout <- bisnode[-train_indices, ]

dim(bisnode_work)
dim(bisnode_holdout)

# separation into train and test automatically via cross-validation using caret package

n_folds = 5
train_control <- trainControl(method = "cv",
                              number = n_folds,
                              classProbs = TRUE, # computing probabilities during training
                              summaryFunction = twoClassSummaryExtended, # from da_helper_functions.R
                              savePredictions = TRUE # save the predictions for each fold
                              )
# train_control$verboseIter <- TRUE # add that for RF

```


<!-- Task1.1: Model 1 - Logit  --> 
<!-- Task1.1: Model 2 - Random forest  --> 
<!-- Task1.1: Model 3 - Logit LASSO?? / Classification Tree?  -->
<!-- Task1.1: Evaluation  -->
```{r}
# No loss function
# RMSE (AUC also?based on threshold that corresponds to prevalence of pos/neg in dataset)
```


<!-- Task1.2: Loss function design -->
<!-- Task1.2: For each of the three models: predict probabilities, look for the optimal classification threshold, calculate expected loss with your loss function  -->
<!-- Task1.2: Pick model with smallest expected loss -->
```{r}

```

<!-- Task1.3: Show a confusion table (on a selected fold or holdout set) -->
<!-- Task1.3: Discuss results, evaluate how useful your model may be -->
```{r}

```

<!-- Task2.1: 2 Samples (service & manufacturing)  --> 

```{r}

```
<!-- Task2.2: Define single loss function (Same loss function as before?) but use for samples separately (pick one prediction model) --> 

```{r}

```

<!-- Task2.3: Compare the model performance across two samples -->

```{r}

```

<!-- Task3.1: Use some methods to interpret your random forest probability model (Shapley, PDP) --> 
<!-- Task3.2: Create a shiny (flexdashboard) app for the project where you can show (1) how key variables influence the prediction (!Shapley) and (2) show expected loss based on different thresholds. -->