---
title: "Assignment 3 - Finding fast growing firms"
subtitle: "Course: ECBS6067 - Prediction with Machine Learning for Economists"
author: "Anja Hahn, Teresa Huebel & Anne Valder"
date: "2023-12-14"
output:
  pdf_document: default
  html_document:
  df_print: paged
---

# Summary
The goal of our analysis is to classify fast growing firms in order to identify lucrative investment opportunities. This report includes three different predictive models to predict fast growing firms. It is based on bisnode panel data and firm growth is defined by average annual percentage growth in sales over two years. We use data from 2012 to predict sales growth two years ahead. The models are logit, random forest and logit lasso. The models' predictive power is evaluated once without once with the use of a loss function. After the introduction of a loss function we identified XXX as strongest model. This model is also used on the subsets manufacturing and services In comparison, XXXX. Further results and interactive graphs can be found in the shiny app linked to this report.

{{< pagebreak >}}

```{r, include=FALSE}
# Clear memory
rm(list=ls())

# set global chunk options
knitr::opts_chunk$set(include = FALSE, fig.align = 'center', cache = TRUE,
                      warning = FALSE, message = FALSE, echo = FALSE)

options(scipen = 999)

```

```{r setup, include=FALSE}
# Set options for markdown
knitr::opts_knit$set(root.dir = 'C:/Users/thuebel/OneDrive - WU Wien/Docs/06 Courses and Study/02 Prediction with ML for Economists/da_case_studies')

# store different directories here:
# C:/Users/thuebel/OneDrive - WU Wien/Docs/06 Courses and Study/02 Prediction with ML for Economists/da_case_studies
# C:/Users/avalder/OneDrive - WU Wien/Documents/Study/WS_23_24/Pred_MLE_Econ/da_case_studies
# C:/Users/AnjaHahn/OneDrive - DataScience Service GmbH/WU/Courses/CEU Prediction with Machine Learning for Economists/da_case_studies

```

<!-- Set directory, load functions and theme  --> 
```{r, include=FALSE}
#getwd()
source("set-data-directory.R")
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")


data_in <- paste(data_dir,"bisnode-firms","clean/", sep = "/")
#use_case_dir <- "ch17-predicting-firm-exit/"

#set output directory
output <- "output/"

```

```{r,include=FALSE, warning=FALSE}
#load libraries
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
library(modelsummary)
library(treeshap)
library(summarytools)
library(pROC)
# add
```

<!-- Load data  --> 
```{r,include=FALSE}
bisnode <- read_csv(paste(data_in,"cs_bisnode_panel.csv", sep = "/")) # 287829 obs. of 48 variables 
```

<!-- Data summary  --> 
```{r, include=FALSE}
# count missing values
colSums(is.na(bisnode))
colSums(is.na(bisnode))/nrow(bisnode)*100

#proportion of observations with less than 1 labor_avg among all that have an observation for labor_avg
bisnode %>% filter(labor_avg < 1) %>% nrow() / bisnode %>% filter(!is.na(labor_avg)) %>% nrow()

# look at data
bisnode %>% glimpse()

# look at character variables
bisnode %>%
  group_by(gender) %>%
  summarise(n = n(), mean = mean(sales))

# overall summary statistics
 #descr(bisnode)

```


## Sample Design

We exclude some variables that have extremely high rates of missing values (above 90%) and only look at firms that are active, that is, they have some sales. What is more, our analysis is only based on firms with sales between 1000 and 10 million euros. We believe this subset presents the most attractive investment opportunities. First, very small firms with sales below 1000 EUR in sales are likely not to seek investment and high growth events might happen at random more easily. Second, very large firms are well represented in various indices and any expected growth is likely to be already incorporated in the stock price. Hence, our sample selection represents a business decision to focus on a segment that is more predictable and more promising for investors.

<!-- Sample design  --> 

```{r, include=FALSE}
####################################
# Sample design
####################################
# collect "weird stuff" here:
# Company with ID 12212152320 had a massive explosion from 2012 to 2013 (0 to 63 employees etc) - how to treat?
## did they exit before? Maybe we have to add a variable that indicate newly established firms?

# add all missing year and company combinations
bisnode <- bisnode %>%
  complete(year, comp_id)

# drop variables with many NAs
bisnode <- bisnode %>%        
  select(-c(COGS, finished_prod, net_dom_sales, net_exp_sales, wages, D))

# find the years with the least missing sales values
bisnode %>%
  group_by(year) %>%
  summarise(missing = sum(is.na(sales))) %>%
  arrange(missing)
# biggest sample would be 2013-2014
# NAs of all variables per year 
bisnode %>%
  group_by(year) %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100, .names = "na_share_{.col}")) %>%
  ungroup()

# generate status_alive, we only want to analyse active firms
bisnode  <- bisnode %>%
  mutate(status_alive = sales > 0 & !is.na(sales) %>%
           as.numeric(.))

# look at cross section
# add explanation to report!!!!
bisnode <- bisnode %>%
  filter(status_alive == TRUE) %>% # only look at firms that have some sales
  # look at firms below 10m euro revenues and above 1000 euros
  filter(!(sales > 10000000)) %>%
  filter(!(sales < 1000))

```


<!-- Data visualization  --> 

```{r, include=FALSE}
# visualize sales
plot_sales <-  ggplot(bisnode, aes(x = sales)) +
  geom_histogram(bins = 100, fill = color[1], color = color.outline, alpha = 0.8) +
  scale_x_log10() +
  labs(x = "Sales (log)", y = "Frequency")+
  theme_bg() 
plot_sales

# visualize labor_avg
plot_labor_avg <- bisnode %>% filter(labor_avg > 0) %>%
  ggplot(aes(x = labor_avg)) +
  geom_histogram(bins = 100, fill = color[1], color = color.outline, alpha = 0.8) +
  labs(x = "Average Number of Employees", y = "Frequency") +
  xlim(0, 5) +
    theme_bg() 
plot_labor_avg


```


## Label Engineering

We are exploring multiple alternatives as target variable to represent high growth firms. In our selection we pay attention to the following dimensions: i) the indicator to choose, ii) the measurement, iii) the time horizon, and iv) the threshold. The literature does not yield one consensus over a definition of high growth firms, it rather points out that any decision along these dimensions depends on the goal of the analysis.[^1] We decided for relative average annual sales growth over two years for the following reasons: First, sales provided a more relevant and reliable target than number of employees. Sales data is missing only in 2.6% of cases whereas number of employees is missing in 50.9% of cases. What is more, 85% of firms had less than 1 employee on average. Second, we decided to use relative growth instead of absolute growth. This decision will favor smaller firms since high relative growth is harder to achieve for larger firms. We believe relative growth to be more relevant since it is more closely linked to our expected return on investment. To be more specific, we calculated percentage change and did not rely on log differences. Even though this decision does not affect the ranking of firms, it does affect the magnitude of the growth rate in the subset of high growth firms. We favored percentage change because of its more intuitive interpretation. Third, we decided for a two year time horizon instead of a one year horizon to relate more closely to the OECD definition of high growth firms that is defined over a three year horizon. 
Third, we decided for a threshold of 30% annual growth. In our sample this is equivalent to classifying the fastest growing 20% of firms as HGF. In the literature, both thresholds on a certain growth rates and a certain percentile of firms are used. We decided for a threshold based on a specific growth rates since we will analyze different sectors (manufacturing and services) with a fixed benchmark.

[^1] Alex Coad, Sven-Olov Daunfeldt, Werner Hölzl, Dan Johansson, Paul Nightingale, High-growth firms: introduction to the special section, Industrial and Corporate Change, Volume 23, Issue 1, February 2014, Pages 91–112, https://doi.org/10.1093/icc/dtt052

<!-- Label engineering  --> 

```{r, include=FALSE}
####################################
# Label engineering
####################################

# generate different variations to display sales
bisnode <- bisnode %>%
  filter(status_alive == TRUE) %>% # only look at firms that have some sales
  mutate(ln_sales = log(sales), # if we want to include firms not_alive need to specially treat 0
         sales_mil=sales/1000000,
         sales_mil_log = ifelse(sales > 0, log(sales_mil), 0))
# this already windsorizes sales

# generate different variations to display labor_avg
bisnode <- bisnode %>%
  mutate(ln_labor_avg = ifelse(labor_avg > 0, log(labor_avg), 0))

###################################
# Creation of different labels: sales
###################################

# YoY relative sales growth
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(sales_growth_ahead = (lead(sales)/sales -1)) %>%
  ungroup()

# YoY sales growth as logdiff
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(d1_sales_mil_log = lead(sales_mil_log) - sales_mil_log ) %>%
  ungroup()

# 2-year ahead average annual sales growth as percentage change
# our target!!!
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(sales_growth_2y_ahead = (lead(sales, n = 2L)/sales -1)/2) %>%
  ungroup()

###################################
# Visualization sales growth

# make histogram that shows both sales growth and d1_sales_mil_log
plot_sales_growth_both <- ggplot(bisnode, aes(x = sales_growth_ahead)) +
  geom_histogram(aes(fill = "Percentage Change"), bins = 100, color = color.outline, alpha = 0.5) +
  geom_histogram(aes(x = d1_sales_mil_log, fill = "Log-Differences"), bins = 100, 
                 color = color.outline, alpha = 0.5) +
  scale_x_log10() +
  labs(x = "Sales growth (log)", y = "Frequency") +
  scale_fill_manual(values = c("Percentage Change" = color[1], "Log-Differences" = color[2]), 
                    name = "Growth Measures") +
  theme_bg()

plot_sales_growth_both
# It makes a difference whether relative growth is measured in log(diff) or percentage change
# since logdiff is an inaccurate approximation for high changes this is especially relevant for high growth firms

###################################
# Creation of different labels: employment
###################################
# important note: employee variables are now calculated based on lag variabes not lead variables
# hence, we can use them as features instead of target

# YoY relative employee growth
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(labor_growth = (labor_avg/lag(labor_avg) - 1)) %>%
  ungroup()

# YoY employee growth as logdiff
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(d1_labor_avg_log = ln_labor_avg - lag(ln_labor_avg) ) %>%
  ungroup()

# birch index (weighted index of absolute and relative employment growth)
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(birch_ind = ((labor_avg - lag(labor_avg)) * (labor_avg/lag(labor_avg) - 1)) ) %>%
  ungroup()

###################################
# Visualization employment growth

#make histogram that shows both employee growth and d1_labor_avg_log
plot_labor_growth_both <- ggplot(bisnode, aes(x = labor_growth)) +
  geom_histogram(aes(fill = "Percentage Change"), bins = 100, color = color.outline, alpha = 0.5) +
  geom_histogram(aes(x = d1_labor_avg_log, fill = "Log-Differences"), bins = 100, 
                 color = color.outline, alpha = 0.5) +
  labs(x = "Employee growth (log)", y = "Frequency") +
  scale_fill_manual(values = c("Percentage Change" = color[1], "Log-Differences" = color[2]), 
                    name = "Growth Measures") +
  scale_x_log10() #+
  # theme_bg()
plot_labor_growth_both

# make histogram of birch index
plot_birch_ind <- ggplot(bisnode, aes(x = birch_ind)) +
  geom_histogram(bins = 100, fill = color[1], color = color.outline, alpha = 0.8) +
  labs(x = "Birch Index", y = "Frequency") +
  theme_bg() +
  scale_x_log10()
plot_birch_ind


###################################
# Overwrite d1_sales_mil log to lagged variables since we will not use it as target but might include it as predictor
# YoY sales growth as logdiff
bisnode <- bisnode %>%
  group_by(comp_id) %>%
  arrange(year) %>%
  mutate(d1_sales_mil_log = sales_mil_log - lag(sales_mil_log) ) %>%
  ungroup()


# show the number of NA in the variable growth_2y_ahead for each year
x <-bisnode %>%
  group_by(year) %>%
  summarise(na_count = sum(is.na(sales_growth_2y_ahead)))


###################################
# Define Cutoff for High Growth firms
###################################

# Look at good cutoff for high growth
quantile(bisnode$sales_growth_2y_ahead, na.rm = TRUE, probs = seq(0, 1, 0.05))

# define high growth firms as firms that grow with more than 30% (coincides with fastest 20% in our data set)
bisnode <- bisnode %>%
  mutate(high_growth = ifelse(sales_growth_2y_ahead > 0.3, 1, 0)) %>%
  # code NA as 0 since these firms have no sales data/below 1000 sales
  # even if the missing values don't mean the firm exited, they are unlikely to be high growth
  # we want to avoid false positives and excluding them would be a bigger issue!
  mutate(high_growth = ifelse(is.na(high_growth), 0, high_growth))
# TARGET VARIABLE: high_growth

# add factor variant of high_growth
bisnode$high_growth_factor <- as.factor(ifelse(bisnode$high_growth == 1, "high_growth", "low_growth"))
# order levels such that low_growth is first (this way it will be used as control in the roc function to determine the loss)
bisnode$high_growth_factor <- factor(bisnode$high_growth_factor, levels = c("low_growth", "high_growth"))


####################################
# notes on growth
####################################
# different types of growth
# yoy relative growth in sales
# average yoy relative growth in sales over 2 years
# yoy relative growth in employees
# average yoy relative growth in employees over 2 years

# cutoff:
# classify as "fast" if they meet a combination of the above?
# OECD definition: 20% average growth in sales or employees over 3 years
# do the same but only with 2 years? Anne: sounds good and justified to me!

# in literature it has been suggested to use fastest growing percintile 8e.g. to 5% of firms)
# see: https://www.diva-portal.org/smash/get/diva2:605658/FULLTEXT01.pdf
# rather than hard cutoff --> but not meaningful for us if we want to compare growth in different industries
# a lot of firms with 0-1 employees: might need a lower absolute threshold for growth such that tiny firms that
# add one extra employee are not classified as fast growing?
```



^23 <!-- Filter for 2012  --> 

```{r}
# restrict sample to 2012 for further analysis

bisnode <- bisnode %>%
  filter(year == 2012)

```


## Feature Engineering

Feature engineering of the bisnode data set comprises re-coding of existing variables s.t. they get the appropriate class and/or meaningful levels and values. Also, plausibility checks are conducted: As some of our financial variables (e.g. *inventories*, *fixed_assets* etc.) must not be negative, we flag them and set negative values to zero. Other implausible variables, such as  *ceo_age* below 18 years, we set to NA. 

In the process of analysing missing values, we impute some important ones (*ceo_age*, *labor_avg*) and add flags for missing values. For other, very important variables (mainly on assets, ceo and industry information), we do not impute but rather drop missing values. Moreover, we intend to use the variable on previous firm growth as predictor for (future) fast growth -- very much in the same fashion as we know from time trends models. Thus, we include the lagged growth in sales (2011 to 2012). As this results in some NAs (due to some missing sales values in 2011), we again flag the observations concerned and impute them by replacing them with 0. This means we assume that said observations did not experience any growth. Although this solution is not ideal, it is based on the assumption that we compare a firm's (missing) sales value in 2011 to the value of its "nearest neighbour" -- the firm's sales in 2012.

Moreover, we compute some new variables: *age* of the company, a dummy for whether it is a *new* company, has *multiple_ceo*s, a *foreign_management*. We classify the observations based on their two-digit NACE codes into "manufacturing" vs. "services" (*ind2_cat* variable), generate a variable for *total_assets* and compute ratios of some financial variables (with either *total_assets* or *sales* as reference). Lastly, we windsorize tails of numeric variables and add flags accordingly.


<!-- Feature engineering  --> 

```{r, include=FALSE}
# re-code one character variable
bisnode <- bisnode %>%
  mutate(urban_m = case_when(
      urban_m == 1 ~ "capital_city",
      urban_m == 2 ~ "other_big_city",
      urban_m == 3 ~ "other"))

# convert character variables to factors 
bisnode <- bisnode %>% 
  mutate_if(is.character, as.factor)

# classify industry categories as factors
bisnode <- bisnode %>%
  mutate(across(all_of(c("ind", "ind2", "nace_main")), as.factor))

###################################
# new variables
# firm characteristics
bisnode <- bisnode %>%
  mutate(age = (year-founded_year) %>% # maybe remove again if computed earlier
           ifelse(. < 0, 0, .),
         age_quad = age^2,
         new = as.numeric(age <= 1),
         foreign_management = as.numeric(foreign >= 0.5),
         multiple_ceo = as.numeric(ceo_count > 1),
         ceo_age = (year-round(birth_year))) # avoid digits for years

###################################
# plausibility checks of relevant numeric variables
for (var in c("age", "ceo_age", "ceo_count", "tang_assets",
             "curr_assets", "curr_liab", "extra_exp", "extra_inc",
             "extra_profit_loss", "fixed_assets", "inc_bef_tax",
             "intang_assets", "inventories", "liq_assets", "material_exp",
             "personnel_exp", "profit_loss_year", "share_eq", "subscribed_cap")){
  print(paste(paste0(var, 
                     ": min:", 
                     min(bisnode[, var], na.rm = TRUE), 
                     "; max:"),
              max(bisnode[, var], na.rm = TRUE)))
}
rm(var)

###################################
# problematic data
# age CEO cannot be negative, also some too young
bisnode <- bisnode %>%
  mutate(ceo_age = ifelse(ceo_age < 18, NA, ceo_age)) # overwrite implausible ones with NA for now

# there cannot be negative financial variables
zero <- c("curr_liab", "curr_assets", "extra_exp", "extra_inc", "fixed_assets",
          "intang_assets", "inventories", "liq_assets", "material_exp",
          "personnel_exp", "subscribed_cap", "tang_assets")

# flag negative ones
for (var in zero){
  aux <- paste0(var, "_flag_error")
  bisnode[, aux] <- as.numeric(bisnode[, var] < 0)
}

# flag any negative assets 
# (extra variable for calculating total assets later)
bisnode <- bisnode %>%
  mutate(flag_asset_problem = ifelse(intang_assets < 0 | curr_assets < 0 | fixed_assets < 0, 1, 0))

# set negative ones to zeros
bisnode <- bisnode %>%
  mutate_at(vars(zero), funs(ifelse(. < 0, 0, .)))


###################################
# NA analysis and imputation
colSums(is.na(bisnode))/nrow(bisnode)*100

# drop missing important variables
# drops about 20%
bisnode <- bisnode %>%
  filter(!is.na(liq_assets), # representative of missing assets data
         !is.na(foreign), # representative of missing CEO data
         !is.na(ind), # representative of industry data (removes many NACE 2-digit categories)
         !is.na(region_m), # geographical info
         !is.na(age),
         !is.na(material_exp)) 


# impute important ones
# ceo age
bisnode <- bisnode %>%
  mutate(flag_missing_ceo_age = as.numeric(is.na(ceo_age)),
         ceo_age = round(ifelse(is.na(ceo_age), mean(ceo_age, na.rm = TRUE), ceo_age)))

# number of employees
bisnode <- bisnode %>%
  mutate(flag_miss_labor_avg = as.numeric(is.na(labor_avg)),
         labor_avg = ifelse(is.na(labor_avg), mean(labor_avg, na.rm = TRUE), labor_avg))

# overwrite ln_labor_avg with imputed values
bisnode <- bisnode %>%
  mutate(ln_labor_avg = ifelse(labor_avg > 0, log(labor_avg), 0))

# impute d1_sales_mil_log as zero growth and create a flag
bisnode <- bisnode %>%
  mutate(flag_missing_d1_sales_mil_log = as.numeric(is.na(d1_sales_mil_log)),
         d1_sales_mil_log = ifelse(is.na(d1_sales_mil_log), 0, d1_sales_mil_log))


# still existing NAs

for (var in names(bisnode)){
  
  if(any(is.na(bisnode[, var]))){
    print(paste0(var, ":", sum(is.na(bisnode[, var]))/nrow(bisnode)*100))
  }
}
rm(var)

###################################
# new variables and 
# variable transformation

# group some industry category codes (2-digit NACE)
# after NA removal, categories have decreased considerably to
# manufactoring and services industries
table(bisnode$ind2)
bisnode <- bisnode %>%
  mutate(ind2_cat = factor(case_when(
    ind2 %in% c("26", "27", "28", "29", "30", "33") ~ "manufacturing",
    ind2 %in% c("55", "56") ~ "services"
  )))

# calculate new age ceo variable
bisnode <- bisnode %>%
  mutate(ceo_young = as.numeric(ceo_age < 40 & !is.na(ceo_age)),
         ceo_old = as.numeric(ceo_age > 75 & !is.na(ceo_age)))

# generate total assets
bisnode <- bisnode %>%
  mutate(total_assets = intang_assets + curr_assets + fixed_assets)

# compute relative variables (sales as reference)
bisnode <- bisnode %>%
  mutate_at(vars("extra_exp", "extra_inc", "extra_profit_loss", 
                 "inc_bef_tax", "inventories", "material_exp",
                 "profit_loss_year", "personnel_exp"),
            funs("rel" = ./sales))

# compute relative variables (total assets as reference)
bisnode <- bisnode %>%
  mutate_at(vars("intang_assets", "curr_liab", "fixed_assets", 
                 "liq_assets", "curr_assets", "share_eq", 
                 "subscribed_cap", "tang_assets"),
            funs("rel" = ifelse(total_assets == 0, 0, ./total_assets)))


######################################
# creating flags and winsorizing tails

# overwrite selected variables with relative ones
zero <- paste0(zero, "_rel")

# flag very high observations and set them to 1
bisnode <- bisnode %>%
  mutate_at(vars(zero), funs("flag_high"= as.numeric(. > 1))) %>%
  mutate_at(vars(zero), funs(ifelse(. > 1, 1, .)))

# for vars that could be any (also negative), but are mostly between -1 and 1
any <-  c("extra_profit_loss_rel", "inc_bef_tax_rel", "profit_loss_year_rel", "share_eq_rel")

# add flags and set tails to -1/1 and add quadratics
bisnode <- bisnode %>%
  mutate_at(vars(any), funs("flag_low" = as.numeric(. < -1))) %>%
  mutate_at(vars(any), funs(ifelse(. < -1, -1, .))) %>%
  mutate_at(vars(any), funs("flag_high" = as.numeric(. > 1))) %>%
  mutate_at(vars(any), funs(ifelse(. > 1, 1, .))) %>%
  mutate_at(vars(any), funs("flag_zero" = as.numeric(. == 0))) %>%
  mutate_at(vars(any), funs("quad" = .^2))

# dropping flags with no variation
variances <- bisnode %>%
  select(contains("flag")) %>%
  apply(2, var, na.rm = TRUE) == 0
bisnode <- bisnode %>%
  select(-one_of(names(variances)[variances]))

# sales
bisnode <- bisnode %>%
  mutate(sales_mil_log_quad = sales_mil_log^2)

bisnode <- bisnode %>%
  mutate(flag_low_d1_sales_mil_log = ifelse(d1_sales_mil_log < -1.5, 1, 0),
         flag_high_d1_sales_mil_log = ifelse(d1_sales_mil_log > 1.5, 1, 0),
         d1_sales_mil_log_mod = ifelse(d1_sales_mil_log < -1.5, -1.5,
                                       ifelse(d1_sales_mil_log > 1.5, 1.5, d1_sales_mil_log)),
         d1_sales_mil_log_mod_quad = d1_sales_mil_log_mod^2
         )

```


Next, we look at relationships between predictors and the binary response. We use the LOESS (locally estimated scatterplot smoothing) method to fit a smooth curve to the data points. In case the curve shows some clear non-linear behaviour, we add quadratic terms of predictors concerned. These decisions are re-assured by the estimation of simple GLMs, where we explain *high_growth* with the respective numeric variable and its quadratic -- whenever we detect a statistically significant relationship between a quadratic term and the binary response, we add that variable as quadratic predictor as well. From visual inspection of the plots, we can say that in general, asset variables do not show a lot of variation in the response i.e. do not seem to affect *fast_growth* as much.



```{r variable_relations}
########################################
# relationships between variables

# scatterplot computation
plots <- list()

# relevant numeric vars
vars_num <- c(zero, any, c("total_assets", "ceo_age", "age", "labor_avg", "ln_labor_avg",
                           "sales_mil", "sales_mil_log", "d1_sales_mil_log_mod",
                           "d1_sales_mil_log_mod_quad"))

# check relationships
for (var in vars_num){
  
  if(any(is.na(bisnode[, var]))){
    print(paste0(var, " entails NAs")) # check for lowess
  } else{
    plots[[var]] <- ggplot(data = bisnode, aes_string(x = var, y = "high_growth")) +
  geom_point(size = 1, fill = color[2], color = color[2]) +
  geom_smooth(method = "loess", se = F, colour = color[1], size = 1.5, span = 0.9) +
  theme_bg()
  }
}

# plots to show
# example of adding quadratic
plots$ceo_age
# example of not adding quadratic
plots$curr_assets_rel

# check lm with quadratics
glm_results <- list()

for (var in vars_num){
  
  # build the formula dynamically
  formula <- formula(paste0("high_growth ~ ", var, " + I(", var, "^2)"))
  
  glm <- glm(formula, data = bisnode, family = "binomial")
  glm_results[[var]] <- summary(glm)
  
}



```

Moreover, we looked at the relationship between the binary response and binary/categorical predictors. In the plot below, we see, for example, that *high_growth* seems to be more prevalent with a young (defined as < 40 years) CEO. Also, a firm being very young (< 1 age) seems to play a role. As regards possible interactions, the below plot hints towards a relationship between industry category (NACE 2 digit classification) and age of the company, as can be seen by the different slopes of the fitted line per category. 

```{r variable_relations}
# add quadratics for chosen variables (based on plots and glms)
quad <- c("intang_assets_rel", "inventories_rel", "liq_assets_rel", "material_exp_rel",
          "personnel_exp_rel", "subscribed_cap_rel", "inc_bef_tax_rel", 
          "profit_loss_year_rel", "share_eq_rel", "total_assets", "ceo_age", "extra_profit_loss_rel")

bisnode <- bisnode %>%
  mutate_at(vars(quad), funs("quad"= .^2))


# response vs. binary and factor variables
binary_variables <- c("gender", "origin", "ind2", "ind", "ind2_cat", "urban_m",
                      "region_m", "ceo_old", "ceo_young")


# two examples
ggplot(bisnode, aes(x = factor(new), fill = factor(high_growth))) +
  geom_bar(position = "fill", stat = "count") +
  labs(y = "Proportion", fill = "High Growth") +
  theme_bg() +
  scale_fill_manual(values = c(color[1], color[2])) +
  theme(legend.text = element_text(size = 10))
ggplot(bisnode, aes(x = factor(ceo_young), fill = factor(high_growth))) +
  geom_bar(position = "fill", stat = "count") +
  labs(y = "Proportion", fill = "High Growth") +
  theme_bg() +
  scale_fill_manual(values = c(color[1], color[2])) +
  theme(legend.text = element_text(size = 10))

# interactions
ggplot(bisnode, aes(x = age, y = high_growth, color = ind2)) +
  # ggplot(bisnode, aes(x = age, y = high_growth, color = ind2_cat)) +
  theme_bw() + 
  geom_point() + 
  geom_point(alpha = .3, 
             size = .9) +
  geom_smooth(method = "lm") #+ 
  # scale_color_manual(values = c(color[1], color[2]))

# drop unused factor levels
bisnode <- bisnode %>%
  mutate(across(where(is.factor), ~ droplevels(.)))

# check remaining data
colSums(is.na(bisnode))/nrow(bisnode)*100
# datasummary_skim(bisnode, type='numeric', histogram = TRUE)


```

## Variable Selection

#### Logit Model

For the logit, we try out two specifications. Both of them include the sales variables (past growth), engineered variables (i.e. ratios of financial variables, windsorized), the detected (possible) quadratic terms and the flags we created for extreme, missing and potentially erroneous data. Also, both specifications include quality variables (balance sheet information) and information on HR (e.g. CEO information) and the firm itself (e.g. regarding the location and industry). The difference lies in interactions included: Whilst the first specification does not include any interactions, the second one interacts selected variables with the industry variable. This gives us XX variables. 

#### Probability Forest

Since the probability forest can deal with complicated relationships between variables, we do not include any quadratics, interacted, modified and/or  windsorized variables. Thus, the raw financial, quality, sales, HR and firm variables are added. Also, potentially highly correlated variables are considered together. Categorical variables are encoded as factors. This gives us XX variables. 

#### LASSO Logit
For the LASSO logit, we consider the same variables as in the second logit specification (including one additional highly correlated variable). This yields in total XXX vars.


<!-- Task1.1: Define variable groups for differnet models  --> 

```{r}
#####################
# Group Variables

rawvars <-  c("curr_assets", "curr_liab", "extra_exp", "extra_inc", "extra_profit_loss", 
              "fixed_assets", "inc_bef_tax", "intang_assets", "inventories", "liq_assets", 
              "material_exp", "personnel_exp", "profit_loss_year", "share_eq", "subscribed_cap")

qualityvars <- c("balsheet_flag", "balsheet_length", "balsheet_notfullyear")

engvar <- c("total_assets", "fixed_assets_rel", "liq_assets_rel", "curr_assets_rel",
            "share_eq_rel", "subscribed_cap_rel", "intang_assets_rel", "extra_exp_rel",
            "extra_inc_rel", "extra_profit_loss_rel", "inc_bef_tax_rel", "inventories_rel",
            "material_exp_rel", "profit_loss_year_rel", "personnel_exp_rel", 
            "curr_liab_rel")

engvar2 <- c(grep("*quad", names(bisnode), value = TRUE))
engvar2 <- engvar2[engvar2 != "sales_mil_log_quad"]

engvar3 <- c(grep("*flag_low$", names(bisnode), value = TRUE),
             grep("*flag_high$", names(bisnode), value = TRUE),
             grep("*flag_error$", names(bisnode), value = TRUE),
             grep("*flag_zero$", names(bisnode), value = TRUE))
# do not add flag_assets_problem due to missing variation
# add two variables not grepled before
engvar3 <- c(engvar3, c("flag_low_d1_sales_mil_log", "flag_high_d1_sales_mil_log"))

sales <-  c(#"sales_mil_log", # no flag for NA because there are none for sales_mil
            "d1_sales_mil_log_mod", 
            "flag_missing_d1_sales_mil_log")

hr <- c("ceo_old", "ceo_young", "ceo_age", "flag_missing_ceo_age", 
        "gender", "multiple_ceo", "origin", # have origin instead of foreign_management
        "ln_labor_avg", "flag_miss_labor_avg")

firm <- c("age", "new", "urban_m", "region_m", "ind2")

# test <- c(rawvars, qualityvars, engvar, engvar2, engvar3, sales, hr, firm)
# setdiff(names(bisnode), test);setdiff(test, names(bisnode))

# interactions for logit, LASSO
interactions1 <- c("ind2*age", "ind2*age_quad",
                   "ind2*d1_sales_mil_log_mod", 
                   #"ind2*sales_mil_log", 
                   "ind2*ceo_age", "ind2*origin", "ind2*gender",
                   "ind2*urban_m", "ind2*ln_labor_avg", "ind2*multiple_ceo")

# interactions2 <- c("sales_mil_log*age", "sales_mil_log*gender",
                   # "sales_mil_log*profit_loss_year_rel", "sales_mil_log*origin",
                   # "sales_mil_log*multiple_ceo")

####################
# Variable Selection

# logit
# try without interactions?
logitvars1 <- c(qualityvars, engvar, engvar2, engvar3, sales, hr, firm)
length(logitvars1)
logitvars2 <- c(qualityvars, engvar, engvar2, engvar3, sales, hr, firm, interactions1)
length(logitvars2)
logit_model_vars <- list("logit_no_interactions" = logitvars1, "logit_incl_interactions" = logitvars2)

# caret always (no matter if all sales vars excluded or not) gives warnings: 
# Angepasste Wahrscheinlichkeiten mit numerischem Wert 0 oder 1 aufgetreten
# Warnung: Vorhersage durch Fit ohne vollen Rang mag täuschen
# below works with d1_sales_mil_log variables
# below does not work with sales_mil_log variables
# below shows no warnings on multicollinearity
# also, Gabor's code shows warnings: Vorhersage durch Fit ohne vollen Rang mag täuschen
# idea: exclude sales_mil_log variables and ignore warnings
# glm_1 <- glm(formula(paste0("high_growth_factor ~", paste0(logitvars1, collapse = " + "))),
#              family = "binomial",
#              data = bisnode_work)

# summary(glm_1)

# random forest (no interactions, no modified features)
rfvars  <-  c("d1_sales_mil_log", "flag_missing_d1_sales_mil_log", 
              "ceo_count", "labor_avg", "ind", "ind2_cat", rawvars, qualityvars, hr, firm)
length(rfvars)
# for logit LASSO
lassovars <- c("ceo_count", qualityvars, engvar, engvar2, engvar3, sales, hr, firm, interactions1)
length(lassovars)

```


<!-- Preparation for prediction & CV  --> 

```{r,include=FALSE}
set.seed(123456)

# create work and holdout
work_indices <- as.integer(createDataPartition(bisnode$high_growth, p = 0.8, list = FALSE))
bisnode_work <- bisnode[work_indices, ]
bisnode_holdout <- bisnode[-work_indices, ]

dim(bisnode_work)
dim(bisnode_holdout)

# separation into train and test automatically via cross-validation using caret package

n_folds = 5
train_control <- trainControl(method = "cv",
                              number = n_folds,
                              classProbs = TRUE, # computing probabilities during training
                              summaryFunction = twoClassSummaryExtended, # from da_helper_functions.R
                              savePredictions = TRUE, # save the predictions for each fold
                              verboseIter = TRUE)
```

<!-- Task1.1: Model 1 - Logit  --> 

```{r}
#######################################################
# PART I PREDICT PROBABILITIES
# Predict logit model ----------------------------------------------
#######################################################

# No loss function
# RMSE (AUC also?based on threshold that corresponds to prevalence of pos/neg in dataset)

# Train Logit Models ----------------------------------------------
## keep maybe for shiny but decide which is our main model!

CV_RMSE_folds <- list()
logit_models <- list()

for (model_name in names(logit_model_vars)) {

  features <- logit_model_vars[[model_name]]

  set.seed(13505)
  glm_model <- train(
    formula(paste0("high_growth_factor ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = bisnode_work,
    family = binomial,
    trControl = train_control
  )

  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[, c("Resample", "RMSE")]

}


#############################################x
# PART I
# No loss fn
########################################

# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {

  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)

    roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }

  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                              "AUC" = unlist(auc))
}

# For each model: average RMSE and average AUC for models ----------------------------------

CV_RMSE <- list()
CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

# Take best model and estimate RMSE on holdout  -------------------------------------------

best_logit_no_loss <- logit_models[["logit_no_interactions"]]

logit_predicted_probabilities_holdout <- predict(best_logit_no_loss, newdata = bisnode_holdout, type = "prob")
bisnode_holdout[,"best_logit_no_loss_pred"] <- logit_predicted_probabilities_holdout[,"high_growth"]
RMSE(bisnode_holdout[, "best_logit_no_loss_pred", drop=TRUE], bisnode_holdout$high_growth)

```

<!-- Task1.1: Model 2 - Probability forest  --> 

```{r}
#################################################
# Probability forest
# Split by gini, ratio of 1's in each tree, average over trees
#################################################

tune_grid <- expand.grid(
  .mtry = c(5, 6, 7), # sq root is 6.244
  .splitrule = "gini",
  .min.node.size = c(10, 15))

# getModelInfo("ranger")
rf_model_p <- train(
  formula(paste0("high_growth_factor ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = bisnode_work,
  tuneGrid = tune_grid,
  trControl = train_control,
  importance = "impurity"
)

rf_model_p$results

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size

# Get average (ie over the folds) RMSE and AUC ------------------------------------
CV_RMSE_folds[["rf_p"]] <- rf_model_p$resample[,c("Resample", "RMSE")]

auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(Resample == fold)

  roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p"]] <- data.frame("Resample" = names(auc),
                                         "AUC" = unlist(auc))

CV_RMSE[["rf_p"]] <- mean(CV_RMSE_folds[["rf_p"]]$RMSE)
CV_AUC[["rf_p"]] <- mean(CV_AUC_folds[["rf_p"]]$AUC)

# Now use loss function and search for best thresholds and expected loss over folds -----### SECOND PART 


bisnode_work$high_growth_factor


############### LOSS FUNCTION PART missing!

# Take model to holdout and estimate RMSE, AUC and expected loss ------------------------------------

rf_predicted_probabilities_holdout <- predict(rf_model_p, newdata = bisnode_holdout, type = "prob")
bisnode_holdout$rf_p_prediction <- rf_predicted_probabilities_holdout[,"high_growth"]
RMSE(bisnode_holdout$rf_p_prediction, bisnode_holdout$high_growth)

```




<!-- Task1.1: Model 3 - Logit LASSO  -->
```{r}
# Logit lasso -----------------------------------------------------------

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

system.time({
  logit_lasso_model <- train(
    formula(paste0("high_growth_factor ~", paste0(lassovars, collapse = " + "))),
    data = bisnode_work,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )
})

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
#write.csv(lasso_coeffs, paste0(output, "lasso_logit_coeffs.csv"))

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]

# AUC
auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    logit_lasso_model$pred %>%
    filter(Resample == fold)

  roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["LASSO"]] <- data.frame("Resample" = names(auc),
                                      "AUC" = unlist(auc))

CV_RMSE[["LASSO"]] <- mean(CV_RMSE_folds[["LASSO"]]$RMSE)
CV_AUC[["LASSO"]] <- mean(CV_AUC_folds[["LASSO"]]$AUC)

```


<!-- Task1.1: Evaluation  -->
```{r}

# discrete ROC (with thresholds in steps) on holdout -------------------------------------------------
# continuous ROC on holdout with best model (Logit 4) -------------------------------------------
# Confusion table with different thresholds ----------------------------------------------------------
# Calibration curve -----------------------------------------------------------

## ADD
```


<!-- Task1.2: Loss function design -->


```{r}

#############################################
# PART II.
# We have a loss function
########################################

# Introduce loss function
# relative cost of of a false negative classification (as compared with a false positive classification)
FP= 1
FN= 5
cost = FN/FP
# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevalence = sum(bisnode_work$high_growth)/length(bisnode_work$high_growth)

# Draw ROC Curve and find optimal threshold with loss function --------------------------


################################
# incorporate loss function for logit models
best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {

  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")

  best_tresholds_cv <- list()
  expected_loss_cv <- list()

  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    # Check if length(cv_fold$high_growth) is zero
    if (length(cv_fold$high_growth) == 0) {
      print(paste("Warning: length of high_growth in", fold, "of model", model_name, "is zero."))
      next  # Skip to the next iteration
    }

    roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevalence))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$high_growth)
  }

  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv))
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv))

  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]

}

###############################################
logit_summary <- data.frame("Avg of optimal thresholds" = unlist(best_tresholds),
                             "Threshold for Fold5" = sapply(logit_cv_threshold, function(x) {x$threshold}),
                             "Avg expected loss" = unlist(expected_loss),
                             "Expected loss for Fold5" = unlist(logit_cv_expected_loss))

kable(x = logit_summary, format = "latex", booktabs=TRUE,  digits = 3, row.names = TRUE,
      linesep = "", col.names = c("Avg of optimal thresholds","Threshold for fold #5",
                                  "Avg expected loss","Expected loss for fold #5"))

## Create plots based on Fold5 in CV ----------------------------------------------

 for (model_name in names(logit_cv_rocs)) {

   r <- logit_cv_rocs[[model_name]]
   best_coords <- logit_cv_threshold[[model_name]]
   createLossPlot(r, best_coords,
                  paste0(model_name, "_loss_plot"))
   createRocPlotWithOptimal(r, best_coords,
                            paste0(model_name, "_roc_plot"))
 }

# Pick best model based on average expected loss ----------------------------------

best_logit_with_loss <- logit_models[["logit_incl_interactions"]]
best_logit_optimal_treshold <- best_tresholds[["logit_incl_interactions"]]

logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = bisnode_holdout, type = "prob")
bisnode_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"high_growth"]

# ROC curve on holdout
roc_obj_holdout <- roc(bisnode_holdout$high_growth, bisnode_holdout[, "best_logit_with_loss_pred", drop=TRUE])

# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(bisnode_holdout$high_growth)
expected_loss_holdout

# Confusion table on holdout with optimal threshold
holdout_prediction <-
  ifelse(bisnode_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "low_growth", "high_growth") %>%
  factor(levels = c("low_growth", "high_growth"))
cm_object3 <- confusionMatrix(holdout_prediction,bisnode_holdout$high_growth_factor)
cm3 <- cm_object3$table
cm3



```

<!-- Task1.1: Loss function RF -->

```{r}
################################
# loss function for RF
################################

# Now use loss function and search for best thresholds and expected loss over folds -----
best_tresholds_cv <- list()
expected_loss_cv <- list()

for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(mtry == best_mtry,
           min.node.size == best_min_node_size,
           Resample == fold)

  roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
  best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                          best.method="youden", best.weights=c(cost, prevalence))
  best_tresholds_cv[[fold]] <- best_treshold$threshold
  expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$high_growth)
}

# average
best_tresholds[["rf_p"]] <- mean(unlist(best_tresholds_cv))
expected_loss[["rf_p"]] <- mean(unlist(expected_loss_cv))


# display RF performance
rf_summary <- data.frame("CV RMSE" = CV_RMSE[["rf_p"]],
                         "CV AUC" = CV_AUC[["rf_p"]],
                         "Avg of optimal thresholds" = best_tresholds[["rf_p"]],
                         "Threshold for Fold5" = best_treshold$threshold,
                         "Avg expected loss" = expected_loss[["rf_p"]],
                         "Expected loss for Fold5" = expected_loss_cv[[fold]])

kable(x = rf_summary, format = "latex", booktabs=TRUE,  digits = 3, row.names = TRUE,
      linesep = "", col.names = c("CV RMSE", "CV AUC",
                                  "Avg of optimal thresholds","Threshold for fold #5",
                                  "Avg expected loss","Expected loss for fold #5"))


# Create plots - this is for Fold5
createLossPlot(roc_obj, best_treshold, "rf_p_loss_plot")
createRocPlotWithOptimal(roc_obj, best_treshold, "rf_p_roc_plot")

# Take model to holdout and estimate RMSE, AUC and expected loss ------------------------------------

rf_predicted_probabilities_holdout <- predict(rf_model_p, newdata = bisnode_holdout, type = "prob")
bisnode_holdout$rf_p_prediction <- rf_predicted_probabilities_holdout[,"high_growth"]
RMSE(bisnode_holdout$rf_p_prediction, bisnode_holdout$high_growth)

# ROC curve on holdout
roc_obj_holdout <- roc(bisnode_holdout$high_growth_factor, bisnode_holdout[, "rf_p_prediction", drop=TRUE])

# AUC
as.numeric(roc_obj_holdout$auc)

# Get expected loss on holdout with optimal threshold
holdout_treshold <- coords(roc_obj_holdout, x = best_tresholds[["rf_p"]] , input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(bisnode_holdout$high_growth)
expected_loss_holdout
```


<!-- Task1.2: For each of the three models: predict probabilities, look for the optimal classification threshold, calculate expected loss with your loss function  -->
<!-- Task1.2: Pick model with smallest expected loss -->
```{r}

```

<!-- Task1.3: Show a confusion table (on a selected fold or holdout set) -->
<!-- Task1.3: Discuss results, evaluate how useful your model may be -->
```{r}

```

<!-- Task2.1: 2 Samples (service & manufacturing)  --> 

```{r}
# work sets for the two sub samples 
manu_work <- bisnode_work %>% filter(ind2_cat == "manufacturing")
serv_work <- bisnode_work %>% filter(ind2_cat == "services")

# holdout sets for the two sub samples 
manu_holdout <- bisnode_work %>% filter(ind2_cat == "manufacturing")
serv_holdout <- bisnode_work %>% filter(ind2_cat == "services")
```

<!-- Task2.1: 2 Samples (service & manufacturing) Random forest (WITHOUT LOSS FUNCTION!!!)  --> 
```{r}
#################################################
# Probability forest for manufacturing sample!
# Split by gini, ratio of 1's in each tree, average over trees
#################################################


rf_model_p_manu <- train(
  formula(paste0("high_growth_factor ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = manu_work,
  tuneGrid = tune_grid,
  trControl = train_control,
  importance = "impurity"
)

rf_model_p_manu$results

best_mtry <- rf_model_p_manu$bestTune$mtry
best_min_node_size <- rf_model_p_manu$bestTune$min.node.size

# Get average (ie over the folds) RMSE and AUC ------------------------------------
CV_RMSE_folds[["rf_p_manu"]] <- rf_model_p_manu$resample[,c("Resample", "RMSE")]

auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p_manu$pred %>%
    filter(Resample == fold)

  roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p_manu"]] <- data.frame("Resample" = names(auc),
                                         "AUC" = unlist(auc))

CV_RMSE[["rf_p_manu"]] <- mean(CV_RMSE_folds[["rf_p_manu"]]$RMSE)
CV_AUC[["rf_p_manu"]] <- mean(CV_AUC_folds[["rf_p_manu"]]$AUC)


# Take model to holdout and estimate RMSE, AUC and expected loss ------------------------------------

rf_manu_predicted_probabilities_holdout <- predict(rf_model_p_manu, newdata = manu_holdout, type = "prob")
manu_holdout$rf_p_manu_prediction <- rf_manu_predicted_probabilities_holdout[,"high_growth"]
RMSE(manu_holdout$rf_p_manu_prediction, manu_holdout$high_growth)
```

```{r}
#################################################
# Probability forest for service sample!
# Split by gini, ratio of 1's in each tree, average over trees
#################################################

rf_model_p_serv <- train(
  formula(paste0("high_growth_factor ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = manu_work,
  tuneGrid = tune_grid,
  trControl = train_control,
  importance = "impurity"
)

rf_model_p_serv$results

best_mtry <- rf_model_p_serv$bestTune$mtry
best_min_node_size <- rf_model_p_serv$bestTune$min.node.size

# Get average (ie over the folds) RMSE and AUC ------------------------------------
CV_RMSE_folds[["rf_p_serv"]] <- rf_model_p_serv$resample[,c("Resample", "RMSE")]

auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p_serv$pred %>%
    filter(Resample == fold)

  roc_obj <- roc(cv_fold$obs, cv_fold$high_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p_serv"]] <- data.frame("Resample" = names(auc),
                                         "AUC" = unlist(auc))

CV_RMSE[["rf_p_serv"]] <- mean(CV_RMSE_folds[["rf_p_serv"]]$RMSE)
CV_AUC[["rf_p_serv"]] <- mean(CV_AUC_folds[["rf_p_serv"]]$AUC)



# Take model to holdout and estimate RMSE, AUC and expected loss ------------------------------------

rf_serv_predicted_probabilities_holdout <- predict(rf_model_p_serv, newdata = serv_holdout, type = "prob")
serv_holdout$rf_p_serv_prediction <- rf_serv_predicted_probabilities_holdout[,"high_growth"]
RMSE(serv_holdout$rf_p_serv_prediction, serv_holdout$high_growth)
```

<!-- Task2.2: Define single loss function (Same loss function as before?) but use for samples separately (pick one prediction model) --> 

```{r}

```

<!-- Task2.3: Compare the model performance across two samples -->

```{r}

```

<!-- Task3.1: Use some methods to interpret your random forest probability model (Shapley, PDP) --> 
<!-- RF VAR IMP (all 3) W/OUT LOSS adjust  -->

```{r}
### Variable importance top 10 
# use RF model since performed the best!
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}
```


```{r}
####### CHANGE TO FORESTS WITH LOSS ########
# Assuming you have three data sets named df1, df2, and df3
rf_models <- list(rf_model_p, rf_model_p_manu, rf_model_p_serv)
# Create an empty list to store the plots
rf_model_plots <- list()

# Loop over the random forest models
for (i in seq_along(rf_models)) {
  current_rf_model <- rf_models[[i]]

rf_model_var_imp <- importance(current_rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  #mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  #mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))


# Create plot VAR IMP ALL 
  current_plot <- ggplot(rf_model_var_imp_df, aes(x = reorder(varname, imp), y = imp_percentage)) +
    geom_point(color = color[1], size = 1) +
    geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
    ylab("Importance (Percent)") +
    xlab("Variable Name") +
    coord_flip() +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_minimal() +
    theme(panel.grid.major.y = element_blank(),
          axis.text.x = element_text(size = 6),
          axis.text.y = element_text(size = 6),
          axis.title.x = element_text(size = 6),
          axis.title.y = element_text(size = 6))
  
  # Save the plot
  rf_model_plots[[paste0("rf_model_plot_all_", i)]] <- current_plot

# Create plot VAR IMP TOP 10  
  current_plot <- ggplot(rf_model_var_imp_df[1:10,], aes(x = reorder(varname, imp), y = imp_percentage)) +
    geom_point(color = color[1], size = 1) +
    geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
    ylab("Importance (Percent)") +
    xlab("Variable Name") +
    coord_flip() +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_minimal() +
    theme(panel.grid.major.y = element_blank(),
          axis.text.x = element_text(size = 6),
          axis.text.y = element_text(size = 6),
          axis.title.x = element_text(size = 6),
          axis.title.y = element_text(size = 6))
  
  # Save the plot
  rf_model_plots[[paste0("rf_model_plot_10_", i)]] <- current_plot
}

rf_model_plots$rf_model_plot_all_1
rf_model_plots$rf_model_plot_10_1
rf_model_plots$rf_model_plot_all_2
rf_model_plots$rf_model_plot_10_2
rf_model_plots$rf_model_plot_all_3
rf_model_plots$rf_model_plot_10_3
```

### Variable importance grouped 
##### ADJUST FOR GROUPS 

```{r,echo=FALSE, include=F, warning=F}
##############################
# 2) varimp plot grouped to (8)
##############################
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model_p$finalModel$xNames
#f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
#f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
#f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)

groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
               f_property_type = f_property_type_varnames,
               f_room_type = f_room_type_varnames,
               bathroom = "bathroom",
               n_days_since = "n_days_since",
               accommodates = "accommodates",
               beds = "beds")

rf_model_var_imp_grouped <- group.importance(rf_old$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                            imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_grouped_plot <- ggplot(rf_model_var_imp_grouped_df, aes(x = reorder(varname, imp), y = imp_percentage)) +
  geom_point(color = color[1], size = 1) +
  geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +  # Change to theme_minimal or another theme of your choice
  theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines only
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6))
rf_model_var_imp_grouped_plot
```

<!-- Shapley (all 3) AGAIN W/OUT LOSS adjust  -->

```{r,echo=FALSE, include=FALSE, warning=F}
## SHAP VALUES ##
rfvars

### correct facors chosen?? 

# define one-hot encoding function
dmy <- dummyVars("~ ind + ind2 + ind2_cat + gender + origin + region_m + urban_m", 
                 data = bisnode_holdout, fullRank = TRUE)

# #perform one-hot encoding on data frame

 data_holdout_ohe <- data.frame(predict(dmy, newdata=bisnode_holdout))
# 
# # replace "." character to " " to match model object names
names(data_holdout_ohe) <- gsub(x = names(data_holdout_ohe),
                                pattern = "\\.",
                                replacement = " ")
# # unify model for treeshap
## now porblem appa. with RF? 

rf_model_unified <- ranger.unify(rf_model_p$finalModel, data_holdout_ohe)

# # when: Error in set_reference_dataset(ret, as.data.frame(data)) : 
# # Dataset does not contain all features occurring in the model.
# 
# #model_variable_names <- rf_model$coefnames
# #new_data_variable_names <- colnames(data_holdout_ohe)
# 
# #missing_variables <- setdiff(rf_model$coefnames, new_data_variable_names)
# #if (length(missing_variables) > 0) {
#  # stop(paste("Error: Missing variables in new data:", toString(missing_variables)))
# #}
# # if Error message (add variables from Error here manually)
# # Identify the indices of the columns you want to change
# indices_to_change <- which(colnames(data_holdout_ohe) %in% c("neighbourhood_cleansedBrnshj Husum", "neighbourhood_cleansedVesterbro Kongens Enghave"))
# 
# # Create a vector of new column names for the selected columns
# new_names <- c("neighbourhood_cleansedBrnshj-Husum", "neighbourhood_cleansedVesterbro-Kongens Enghave")
# 
# # Change the column names only for the selected columns
# names(data_holdout_ohe)[indices_to_change] <- new_names
# 
# ## go back to unify model for treeshap
# 
# #rf_model_unified <- ranger.unify(rf_model$finalModel, data_holdout_ohe)
# 
treeshap_res <- treeshap(rf_model_p, data_holdout_ohe[1:500, ]) # delete # for calculation but excluded here for Rmd knitting 
# 
# #treeshap_res <- plot_contribution(treeshap_res, obs = 12)
# 
# #treeshap_featureIMP <- plot_feature_importance(treeshap_res, max_vars = 10)
# 
# #treeshap_inter <- treeshap(rf_model_unified, data_holdout_ohe[1:100, ], interactions = T)
# #summary(treeshap_res)
```
<!-- Save data environment for dashboard  -->

```{r}
save.image(file = "A3.RData")
```


<!-- Task3.2: Create a shiny (flexdashboard) app for the project where you can show (1) how key variables influence the prediction (!Shapley) and (2) show expected loss based on different thresholds. -->