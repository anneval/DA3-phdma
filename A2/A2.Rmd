---
title: "Assignment 2 - Airbnb prediction models"
subtitle: "Course: ECBS6067 - Prediction with Machine Learning for Economists"
author: "Anne Valder"
date: "2023-12-02"
output:
  pdf_document: default
  html_document:
  df_print: paged
---

```{r setup, include=FALSE}
# Set options for markdown
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/WS_23_24/Pred_MLE_Econ/da_case_studies')
```

```{r,include=FALSE, warning=FALSE}
# Clear memory
rm(list=ls())

#load libraries
library(rattle)
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
#library(kableExtra) # for Rmd-knitting to work 
library(xtable)
library(modelsummary)
library(gbm)
library(treeshap)
```
<!-- Set directory, load functions and theme  --> 
```{r, include=FALSE}
#getwd()
source("set-data-directory.R")
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")
```
<!-- Load data  --> 
```{r,include=FALSE}
#  
# Copenhagen 24 September, 2023 (data for prediction)
listings_Sep <- read.csv(paste0(data_dir,"/airbnb/assignment/listings_cph_SEP23.csv"), sep = ",", header = T, stringsAsFactors = F)

# Copenhagen 29 December, 2022 (data for model estimation)
listings_Dec <- read.csv(paste0(data_dir,"/airbnb/assignment/listings_cph_DEC22.csv"), sep = ",", header = T, stringsAsFactors = F)

# check whether the two data set from the different review dates differ 
check_equal_columns <- function(df1, df2) {
  cols_df1 <- names(df1)
  cols_df2 <- names(df2)
  
  if (all.equal(cols_df1, cols_df2)) {
    cat("Both data frames have the same columns.\n")
    return(TRUE)
  } else {
    cat("The data frames have different columns.\n")
    return(FALSE)
    }}

check_equal_columns(listings_Sep, listings_Dec)

# Add a new column to each data set to identify their origin
listings_Sep$origin <- "Sep 23"
listings_Dec$origin <- "Dec 22"

# Combine the data sets
listings <- rbind(listings_Sep, listings_Dec)
# define for saving the data later
use_case_dir <- "A2/"
data_out <- use_case_dir
```
**The report below summarizes pricing strategies for small and mid-size apartments in Copenhagen. Using a data-driven approach, I determine that the average price for apartments hosting 2 to 6 guests is 1166 DKK (= 156 Euro) per night, as predicted by the best-performing model. The driving factor influencing apartment prices on average the most is the number of people an Airbnb can accommodate.** The first part of the report delves into data preparation and pre-processing. Subsequently, the three distinct price prediction models, along with all modeling decisions, are explained. This is followed by a comparison of the results with predicted apartment prices in London. In addition, to further assess the predictive power of the models out of sample, predictions are made not only on a holdout set but also on an additional data set covering a different time frame, essentially mimicking live data. Finally, the report provides a detailed evaluation of the model performance of the best-performing machine learning model, utilizing variance-important measures such as Shapely values and partial dependence plots.

**Data Preparation:** The data for apartments in Copenhagen is taken from [Inside Airbnb](http://insideairbnb.com/get-the-data/). The two selected data sets include the review date 24 September, 2023 and the review date 29 December, 2022. After joining the data sets, I proceed with the *sample design* as follows. First, I drop variables that are of no use to the prediction exercise, e.g. variables containing urls, or specifics about the host etc. Next, I make sure that all variables are cleaned (i.e. remove characters or symbols) and of the correct type and class for the prediction exercise (e.g. conversion to binary, numeric and factors). Moreover, I encode the 'price' variable to have the right format (no commas for thousands). Furthermore, I transform the variable 'amenity', which originally contains a listing of all kinds of amenities a certain Airbnb has, into dummy variables. In order to prevent to obtain over 2000 dummy variables I group the most important amenities together and create approximately 15 dummies. To ensure that the sample relates to the policy question at hand I filter for apartments that can house only 2 to 6 people and that correspond to "standard" property types for representative Airbnb prices. To circumvent that the analysis is distorted by errors I drop extreme values by making sure that the minimum number of nights is at least equal to one. At last, I create the variable 'days since first review' which allows me to analyse approximately for how long a apartment has been used as a Airbnb i.e. a proxy for age.
```{r, echo=FALSE, include=FALSE}
# Sample Design:
#1) drop all variables in listings file that are redundant, lots of url variables or descriptive, or host related variables.

drop_variables <- c("host_thumbnail_url","host_picture_url","listing_url","picture_url","host_url","last_scraped","description", "neighborhood_overview", "host_about", "host_response_time", "name", "host_location","neighbourhood_group_cleansed","source","license","calendar_updated") # drop similar variables to airbnb_london_cleaner.R. Either not needed or only contains NAs..

listings<-listings[ , !(names(listings) %in% drop_variables)]

#2) check if id variables contain at least one alphabetical character, if yes drop those
listings$junk<-grepl("[[:alpha:]]", listings$id)
summary(as.factor(listings$junk))
listings <-subset(listings,junk==FALSE)
listings <- listings[,-61] 

#3) check all classes and variable types of data sets
sapply(listings, class)
sapply(listings, typeof)

#4) remove percentage & dollar signs
remove_symbols <- function(x) {
  #remove either dollar sign or percentage sign
  value_clean <-  gsub("[$%]", "", x)
  # convert value to numeric
   if (length(value_clean) > 1) {
    return(value_clean)
  } else {
    return(as.numeric(value_clean))
  }}

## apply the function to all columns in the data frame

listings_clean <- listings %>% mutate_all(.funs = remove_symbols)

#5) Some of the input values in price are improperly formatted due to the presence of commas (i.e.,) between the numbers. Using the gsub function, we can remove these commas.

listings_clean <- listings_clean %>% 
  mutate(price_n = gsub(",", "", price),
         price_n = as.numeric(price_n))

#6) format binary variables
## function to convert "f" and "t" to 0 and 1
convert_binary <- function(x) {
  ifelse(toupper(x) == "T", 1, ifelse(toupper(x) == "F", 0, x))
}

# apply the function to all columns in the data frame
listings_clean <- listings_clean %>%
  mutate_all(.funs = convert_binary)

#7) add numeric columns from certain columns

vars_to_convert <- c("host_id","id","scrape_id","maximum_nights","minimum_nights","accommodates","bathrooms","review_scores_rating","number_of_reviews","reviews_per_month","beds","host_is_superhost","host_listings_count","host_total_listings_count","host_has_profile_pic","host_identity_verified","latitude","longitude","minimum_minimum_nights","maximum_minimum_nights","minimum_maximum_nights","maximum_maximum_nights","minimum_nights_avg_ntm","maximum_nights_avg_ntm","has_availability","availability_30","availability_60","availability_90", "availability_365","number_of_reviews","bedrooms")

listings_clean <- listings_clean %>% 
    mutate(across(all_of(vars_to_convert), as.numeric))
```

```{r, echo=FALSE, include=FALSE}
#8) create dummies for amenities (would create over 2000 variables). Therefore, group manually most important ones and create some dummies. 
# Extract amenities from the text and create dummy variables
extract_and_group_amenities <- function(data) {
  data %>%
    mutate(amenities = gsub('\\[|\\]|"', '', amenities)) %>%
    separate_rows(amenities, sep = ',\\s*') %>%
    mutate(amenities = trimws(amenities)) %>%
    # Remove strings starting with "\u" in the amenities column
    mutate(amenities = gsub('^\\\\u.*', '', amenities)) %>%
    filter(amenities != "") %>%  # Remove empty strings after removal
    mutate(amenity_type = case_when(
      grepl("Wifi", amenities) ~ "a_Internet",
      grepl("Washer|Dryer|Laundromat nearby|Drying rack for clothing|Iron", amenities) ~ "a_Laundry",
      grepl("Kitchen|Microwave|Refrigerator|Dishes and silverware|Toaster|Freezer|Stove|Oven|Cooking basics",  
            amenities) ~ "a_Kitchen",
      grepl("Espresso machine|French press|Hot water|Coffee maker: pour-over coff#ee", amenities) ~ "a_TeaCoffee",
      grepl("Heating", amenities) ~ "a_Heating",
      grepl("Pets allowed", amenities) ~ "a_Petfriendly",
      grepl("Parking|Paid street parking off premises", amenities) ~ "a_Parking",
      grepl("Balcony|Outdoor furniture|Backyard|Private patio or balcony", amenities) ~ "a_OutdoorArea",
      grepl("Soap|Shampoo|Hair dryer|Shower gel|Body soap|Conditioner", amenities) ~ "a_Toiletry",
      grepl("TV with standard cable|TV|Netflix|Disney+|Chromecast|HBO Max|sound system|Bluetooth", amenities) ~ 
            "a_Entertainment",
      grepl("Changing table|High chair|Baby bath", amenities) ~ "a_Childfriendly",
      grepl("Dedicated workspace", amenities) ~ "a_HomeOffice",
      grepl("Long term stays allowed", amenities) ~ "a_LongTermStay",
      grepl("Self check-in|Keypad", amenities) ~ "a_Selfcheckin",
      grepl("Smoke alarm", amenities) ~ "a_Security",
      grepl("Cats", amenities) ~ "a_cats",
      grepl("Dogs", amenities) ~ "a_dogs",
      TRUE ~ "Other_Group"
    )) %>%
    pivot_wider(names_from = amenity_type, values_from = amenities, values_fn = length, values_fill = 0) %>%
    mutate(across(starts_with("a_"), ~ifelse(. > 0, 1, 0)))
}
listings_clean <- extract_and_group_amenities(listings_clean)
# drops the amenities column automatically by storing into "other_group"
```

```{r, echo=FALSE, include=FALSE}
# adjust sample according to policy question:
#9) filter n accommodate 2-6 according to policy question 
summary(as.factor(listings_clean$accommodates))
listings_clean <- listings_clean %>%
  filter(accommodates >= 2 & accommodates <= 6 )

#10) drop extreme values in terms of duration?
# filter min night at least 1 to delete errors
listings_clean <- listings_clean %>% 
    filter(minimum_nights <= 7 & minimum_nights >= 1)

#11) keep only if  certain "standard" property types for representative Airbnb prices (probably extra cost for special Airbnbs like Boats etc.)
summary(as.factor(listings_clean$property_type))

listings_clean <- listings_clean %>%
  filter(property_type %in% c("Entire rental unit","Entire home","Entire loft","Entire serviced apartment","Entire villa","Entire condo","Private room in condo","Private room in home","Private room in rental unit","Private room in townhouse","Entire townhouse")) %>%
  mutate(f_property_type = factor(property_type))

#12) room type as factor 
table(listings_clean$room_type)
listings_clean <- listings_clean %>%
  mutate(f_room_type = factor(room_type))

#13) rename room type because of length 
listings_clean$f_room_type2 <- factor(ifelse(listings_clean$f_room_type== "Entire home/apt", "Entire/Apt",
                            ifelse(listings_clean$f_room_type== "Private room", "Private",
                                   ifelse(listings_clean$f_room_type== "Shared room", "Shared", "."))))

#14) create days since first review
listings_clean <- listings_clean %>%
  mutate(
    n_days_since = as.numeric(as.Date(calendar_last_scraped,format="%Y-%m-%d") -
                                as.Date(first_review ,format="%Y-%m-%d")))
```
In terms of *label engineering* I assure that minimum and maximum values are reasonable and drop extreme values above 15.000 DKK (i.e. 2000 Euro per night) which are likely to be errors after carefully considering the characteristics of those high-priced Airbnbs. Moreover, I check whether a log-transformation is feasible, since 
it might be interesting to analyse relative changes rather than changes per monetary unit. However, the two graphs in the appendix show that transforming the target variable to logs does not alter the shape of the distribution significantly. Therefore, I continue to use the target variable in levels.   
```{r, echo=FALSE, include=FALSE, warning=FALSE}
### Label engineering
#1) keep Airbnbs with price info only
summary(listings_clean$price_n)
listings_clean <- listings_clean %>%
  drop_na(price)

#2) drop extreme values in terms of price
summary(listings_clean$price_n)
# min is 75 DKK which is approx 10EUR and max is 150368DKK which is 20166,60 EUR
# Drop everything above 15000DKK (i.e. 2000EUR) 
listings_clean <- listings_clean %>% 
  filter(price_n <=  15000) %>% 
  mutate(price_ln = log(price_n))

#3 log price or not look at each distribution 
p1_lnprice <- ggplot(listings_clean, aes(price_ln)) +
  geom_histogram(binwidth = 0.15, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("Count") +
  xlab("Log price") +
  theme_bg()
#p1_lnprice

p2_price <- ggplot(listings_clean, aes(price_n)) +
  geom_histogram(fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("count") +
  xlab("Price") +
  ggplot2::xlim(c(0,5000))+
  theme_bg()
#p2_price
```
Next, I turn to the *feature engineering:* choices. First, I inspect *missing values*. Here I drop variables if they contain to many missing values and are not of importance for further analysis (e.g. calender updated and license). Other variables like 'bathrooms' or 'bedrooms', I impute with the help of descriptive variables like 'bathroom text' or approximate with the variable 'number of beds' and 'accommodates'. Some variables that contain only a few missing variables get the missing values replaced with the median of the non-missing values (e.g. n_days_since, review). 
```{r, echo=FALSE, include=FALSE,warning=FALSE}
#1) check for missing variables:
# calender updated and license many NAs, dropped beginning, drop columns when many missing not important
to_filter <- sapply(listings_clean, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#2a) NAs bathroom use bathrooms_text
listings_clean <- listings_clean %>%
  mutate(bathrooms = as.numeric(str_extract(bathrooms_text, "\\d+\\.?\\d*")),
         shared_bathroom = as.integer(str_detect(bathrooms_text, "shared")))

#2b) imput when few, assume at least 1 bath
listings_clean <- listings_clean %>%
  mutate(
    bathrooms =  ifelse(is.na(bathrooms), median(bathrooms, na.rm = T), bathrooms)) 

#3a) bedroom & number of beds NAs 
## use number of beds or impute from number of bed, if bed = 1 than put 1, either remain NA than check again number NAs: updating the 'bedrooms' variable based on the values in the 'beds' column
listings_clean <- listings_clean %>%
  mutate(bedrooms = if_else(beds %in% c(1, 2), 1, bedrooms)) %>%
  filter(bedrooms < 15) # there was one error 15 bedrooms 5 beds.

#3b) updating the 'beds' variable based on the values in the 'bedrooms'or 'accommodates' column
listings_clean <- listings_clean %>%
  mutate(
    beds = if_else(is.na(beds) & !is.na(bedrooms), bedrooms, beds),
    beds = if_else(is.na(beds) & is.na(bedrooms) & accommodates %in% c(1, 2), 1,
                   if_else(is.na(beds) & is.na(bedrooms) & accommodates > 2, 2, beds)),
    bedrooms = if_else(is.na(bedrooms) & is.na(beds) & accommodates %in% c(1, 2), 1,
                       if_else(is.na(bedrooms) & is.na(beds) & accommodates > 2, 2, bedrooms)))

#4) replace missing variables like reviews with zero, when no review + add flags
listings_clean <- listings_clean %>%
  mutate(
    flag_days_since=ifelse(is.na(n_days_since),1, 0),
    n_days_since =  ifelse(is.na(n_days_since), median(n_days_since, na.rm = T), n_days_since),
    flag_review_scores_rating=ifelse(is.na(review_scores_rating),1, 0),
    review_scores_rating =  ifelse(is.na(review_scores_rating), median(review_scores_rating, na.rm = T),
                                   review_scores_rating),
    flag_reviews_per_month=ifelse(is.na(reviews_per_month),1, 0),
    reviews_per_month =  ifelse(is.na(reviews_per_month), median(reviews_per_month, na.rm = T), reviews_per_month))

# CHECK AGAIN: where do we have missing variables now?
to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0] # should be 0 now 
```
In the following I consider th *functional form* of some of the predictors. However, these feature modifications are only relevant for the OLS (LASSO) model. The other two (machine-learning) models are non-parametric algorithms, which should be able to find interactions between variables and non-linear behavior. After visual (see examples in appendix) and some basic regression analysis I add squared, cubic and log terms for the following variables: accommodates, beds, number_of_reviews, n_days_since, and review_scores_rating. 
```{r, echo=FALSE, include=FALSE}
#1) Feature engineering: functional form
## accommodates: look at distribution
summary(as.factor(listings_clean$accommodates))

listings_clean %>%
  group_by(accommodates) %>%
  summarise(mean_price = mean(price_n), min_price= min(price_n), max_price = max(price_n), n = n())

p3_accommodates <- ggplot(data = listings_clean, aes(x=accommodates, y=price_n)) +
  geom_point(size=1, colour=color[3], shape=16)+
  labs(x="Number of people accomodated",y="Price")+
  geom_smooth(method="lm", colour=color[1], se=FALSE)+
  theme_bg()

#p3_accommodates

## Squares and further values to create
listings_clean <- listings_clean %>%
  mutate(accommodates2=accommodates^2, ln_accommodates=log(accommodates) ,
         ln_accommodates2=log(accommodates)^2)

## Regression: price and number of accommodates and squares
summary(lm(price_n ~ accommodates + accommodates2, data=listings_clean)) # not relevant?
## Regression: price and log number of accommodates
summary(lm(price_n ~ ln_accommodates , data=listings_clean)) # relevant 
## Regression: price and log squares number of accommodates  
summary(lm(price_n ~ ln_accommodates2, data=listings_clean))# relevant 
```

```{r, echo=FALSE, include=FALSE}
## Number of reviews: look at distribution, factorize 
p4_nreview <- listings_clean %>%
  filter(number_of_reviews <100) %>% 
  ggplot( aes(number_of_reviews)) +
  geom_histogram(binwidth = 5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of reviews") +
  theme_bg()
#p4_nreview
## Logs and further values to create
listings_clean <- listings_clean %>%
  mutate(ln_number_of_reviews = log(number_of_reviews+1))

p5_lnreview <- ggplot(listings_clean, aes(ln_number_of_reviews)) +
  geom_histogram(binwidth = 0.5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("Log N of reviews") +
  theme_bg()
#p5_lnreview
# Pool num of reviews to 3 categories: none, 1-51 and >51
#summary(as.factor(listings_clean$number_of_reviews))

listings_clean <- listings_clean %>%
  mutate(f_number_of_reviews = cut(number_of_reviews, c(0,1,51,max(listings_clean$number_of_reviews)), labels=c(0,1,2), right = F))
listings_clean %>%
  group_by(f_number_of_reviews) %>%
  summarise(median_price = median(price_n) ,mean_price = mean(price_n) ,  n=n())
# Regression 1: log-price and number of reviews
summary(lm(price_n ~ f_number_of_reviews, data=listings_clean))
# Regression 2: log-price and log number of reviews
summary(lm(price_n ~ ln_number_of_reviews, data=listings_clean))

```

```{r, echo=FALSE, include=FALSE}
## Beds: look at distribution
p6_beds <-ggplot(listings_clean, aes(beds)) +
  geom_histogram(binwidth = 5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N beds") +
  theme_bg()
#p6_beds
#maybe best is to have log beds
listings_clean %>%
  group_by(beds) %>%
  summarise(mean_price = mean(price_n), min_price= min(price_n), max_price = max(price_n), n = n())
## Logs and further values to create
listings_clean <- listings_clean %>%
  mutate(ln_beds = log(beds),
         beds2=beds^2)
## Regression: price and log (beds)
summary(lm(price_n ~ ln_beds, data=listings_clean)) #relevant
## Regression: price and squared beds
summary(lm(price_n ~ beds2, data=listings_clean)) #relevant
```

```{r, echo=FALSE, include=FALSE}
## bathrooms: look at distribution
p7_bathrooms <- ggplot(listings_clean, aes(bathrooms)) +
  geom_histogram(binwidth = 0.5, fill = color[1], color = color.outline, alpha = 0.8, size = 0.25) +
  ylab("") +
  xlab("N of bathrooms") +
  theme_bg()
#p7_bathrooms
```

```{r, echo=FALSE, include=FALSE}
## create variables, measuring the time since: squared, cubic, logs
listings_clean <- listings_clean %>%
  mutate(
    ln_days_since = log(n_days_since+1),
    ln_days_since2 = log(n_days_since+1)^2,
    ln_days_since3 = log(n_days_since+1)^3 ,
    n_days_since2=n_days_since^2,
    n_days_since3=n_days_since^3,
    ln_review_scores_rating = log(review_scores_rating),
    ln_days_since=ifelse(is.na(ln_days_since),0, ln_days_since),
    ln_days_since2=ifelse(is.na(ln_days_since2),0, ln_days_since2),
    ln_days_since3=ifelse(is.na(ln_days_since3),0, ln_days_since3),
  )

# Check the effect
p8_lndays <- listings_clean %>%
  filter(listings_clean$price_n<=15000, ln_days_since>2) %>%
  ggplot( aes(x=ln_days_since , y=price_n)) +
  geom_point(size=1.5, colour=color[3], shape=4) +
  geom_smooth(method="loess", colour=color[1], se=F)+
  labs(x="Log number of days since first review",y="Log daily price")+
  theme_bg()
#p8_lndays
#-Inf values
summary(lm(price_n ~ ln_days_since + ln_days_since2 + ln_days_since3, data=listings_clean)) # not significant

## review score effect: look at distribution
p9_reviewsr <- ggplot(data = listings_clean, aes(x=review_scores_rating , y=price_n)) +
  geom_point(size=1.5, colour=color[3], shape=4) +
  geom_smooth(method="loess", colour=color[1], se=F)+
  labs(x="Review score",y="Daily price (USD)")+
  theme_bg()
#p9_reviewsr
#Create log of review scores
listings_clean <- listings_clean %>%
  mutate(ln_review_scores_rating = log(review_scores_rating))

# Regression:  price - num of review scores
summary(lm(price_n ~ review_scores_rating,data=listings_clean)) # relevant
# Regression: price - log num of review scores
#summary(lm(price_n ~ ln_review_scores_rating,data=listings_clean))
#leave as is
```
Last, I consider the *interactions of predictors*. Again this is only relevant for the parametric OLS (LASSO) model.I assume interactions with f_property_type, f_room_type and accommodates and visualize those (see box plots in appendix). The interaction follows the believe, the impact of room type on the response variable (e.g., price) varies depending on the specific property type, an interaction term can capture these differential effects. For example, the effect of upgrading to a higher room type might have different implications for prices in different property types (e.g., apartments vs. houses). Moreover, I include interactions between each f_property_type or f_room_type and all the amenity dummies, similar as to the London example. The final data set (including data for review date September 23 only) thus consists of 99 variables with 11880 observations. Regarding the target variable the mean price per night of an Airbnb in Copenhagen is 1184 DKK, the maximum is 15000 DKK and the minimum price is 75DKK.Further summary statistics of the final data set can be found in the markdown file in the chunk: Summary statistics.
```{r, echo=FALSE, include=FALSE}
# Interactions
##1) Box plot of price by room type
summary(as.factor(listings_clean$f_room_type))

p10_roomtypebox <- ggplot(data = listings_clean, aes(x = f_room_type, y = price_n)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               color = c(color[2],color[1]), fill = c(color[2],color[1]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,2500), breaks = seq(0,2500,500)) +
  labs(x = "Room type",y = "Price (DKK)")+
  theme_bg()
#p10_roomtypebox
##2) Box plot of accommodates and price
summary(as.factor(listings_clean$accommodates))

p11_accomdatesbox <- ggplot(listings_clean, aes(x = factor(accommodates), y = price_n))+
 geom_boxplot(aes(group = accommodates),
               color = c(color[2],color[1], color[3], color[4],color[5]), fill = c(color[2],color[1], color[3], color[4],color[5] ),
               size = 0.3, width = 0.8, alpha = 0.3, na.rm=T, outlier.shape = NA)  +
     stat_boxplot(aes(group = accommodates), geom = "errorbar", width = 0.8,
               color = c(color[2],color[1], color[3], color[4],color[5] ), size = 0.5, na.rm=T)+
    scale_color_manual(name="",
                     values=c(color[2],color[1],color[1],color[2],color[1],color[2])) +
  scale_fill_manual(name="",
                     values=c(color[2],color[1],color[3],color[4],color[1],color[2])) +
  labs(x = "Accomodates (Persons)",y = "Price (DKK)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 3500), breaks = seq(0,3500, 500))+
  theme_bg() +
  theme(legend.position = c(0.3,0.8)        )
#p11_accomdatesbox
```

```{r, echo=FALSE, include=FALSE}
# interactions of factors and dummies
amenities <-  grep("^a_.*", names(listings_clean), value = TRUE)
X1  <- c("f_room_type*f_property_type")
X2  <- c(paste("(f_property_type + f_room_type * (",paste(amenities, collapse=" + "),"))"))

```

<!-- Save final data set for back up !--> 
```{r, echo=FALSE, include=FALSE}
#write.csv(listings_clean, paste0(data_out, "airbnb_cph_work_book.csv"), row.names = F)
```

<!-- Descriptive statistics of final data set !--> 
```{r Summary statistics, echo=FALSE, include=FALSE}
### Descriptive stats of final data set
skimr::skim(listings_clean) #kable(skimr::skim(listings_clean)) - Appendix 

dim(listings_clean)


# descriptive of target variable
Hmisc::describe(listings_clean$price_n)
summary(listings_clean$price_n)

# descriptive of some factor variables
describe(listings_clean$f_room_type)
describe(listings_clean$f_property_type)
table(listings_clean$f_number_of_reviews)

#How is the average price changing in my district by `property_type`, `room_type`?
listings_clean %>%
  group_by(f_property_type, f_room_type) %>%
  dplyr::summarize(mean_price = mean(price_n, na.rm=TRUE))
```

**Model selection:** I chose to conduct the analysis using first an *LASSO model*, second a *Random forest model* and last a *Boosting algorithm*. In the following I will go trough each model argue for its choice and modelling decisions and then  compare their performance. To begin the prediction exercise I start by separating my data set several times. First, I filter so that it only contain the data from the first review date (29 December, 2022). Second, I separate the data into a holdout set for evaluation and a working data set to estimate the three different models. The working data set gets further divided into a training and a test set for cross validation when 'calibrating' the right model parameters. This gets done automatically via the 'train' function in the caret package, when used with cross-validation (method = "cv").
```{r, echo=FALSE, include=FALSE}
# Data set:
#1) for task 1 we only need the data from September 2023 
listings_clean_SEP <- listings_clean[listings_clean$origin == "Sep 23",]
listings_clean <- listings_clean[listings_clean$origin == "Dec 22",]

#2) separate data into holdout and work data set of the September 2023 data 
# use only ~80% as working data, rest is hold out set for evaluation
smp_size <- floor(0.2 * nrow(listings_clean))
# Set the random number generator, once enough (rmd?)
set.seed(123456789) 
# seq_len: generate regular sequences
# sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(listings_clean)), size = smp_size) # create ids
listings_clean$holdout <- 0 # new column 'holdout' all 0 at first
listings_clean$holdout[holdout_ids] <- 1 #if holdout id is 1, add 1 to column 'holdout'

#Hold-out set (September 2023 data )
listings_holdout <- listings_clean %>% filter(holdout == 1)

#Working data set (September 2023 data)
listings_work <- listings_clean %>% filter(holdout == 0)

dim(listings_work)
dim(listings_holdout)

#### Do not need to Split the working set 'by hand' into train and test set since I estimate the models using 5-fold cross-validation. So the 'train' function in the caret package, when used with cross-validation (method = "cv"), automatically splits the data into different training and testing sets for each fold (here 5). 
# do 5-fold CV
n_folds = 5
train_control <- trainControl(method = "cv",
                              number = n_folds,
                              verboseIter = TRUE) #the training process will print information about each iteration, which can include details such as the iteration number, the performance metric value, and other relevant information.
```

```{r,include=FALSE}
# Model set up (more relevant if I would have tries different OLS versions like in the London example, but however keep for nicer structure)
# Basic Variables
basic_vars  <- c("accommodates", "beds", "f_property_type", "f_room_type", "n_days_since", "flag_days_since","neighbourhood_cleansed","bathrooms")
reviews <- c("f_number_of_reviews","review_scores_rating", "flag_review_scores_rating")
# Higher orders
poly_vars <- c("accommodates2", "n_days_since2", "n_days_since3")

# define predictors for LASSO/ OLS Model or the Machine Learning models (exlude interactions,sqares...)
predictors_LASSO <- c(basic_vars, reviews,poly_vars, amenities, X1,X2)
predictors_ML <- c(basic_vars, reviews, amenities)
```
<!-- Model 1: LASSO !--> 
The first model is the *LASSO model*. I chose it in order to not fully depend on domain knowledge and to not be too concerned with variable selection but instead let the model itself choose the most important predictors automatically. Moreover, this way potential overfitting is avoided. I set a tuning grid to do cross validation and choose the value for lambda (between 0.5 and 1), the parameter that sets the strength of the variable selection. The larger the lambda the more aggressive is the selection and thus fewer variables are left in the regression. The best performing LASSO model has a lambda of 0.75. From all possible predictors (including interactions, polynomials etc.) the model shrinks the weakest coefficients to zero to reduce variance and ends up picking 61 out of 74 predictors and (see 'lasso_coeffs_nz' in the LASSO chunk in the markdown file) and has a RMSE of 725.6417.
```{r LASSO, echo=FALSE, include=FALSE, warning=F}
# Set lasso tuning parameters
tune_grid_lasso <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

lasso_model <- caret::train( formula(paste0("price_n ~", paste0(predictors_LASSO, collapse = " + "))),
                      data = listings_work,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid_lasso,
                    na.action=na.exclude)

lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed

print(lasso_model$bestTune$lambda) # cv choice of lambda
print(lasso_coeffs)

lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz)) # keep 61 from 74

summary(lasso_model)

# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
```
<!-- Model 2: Random forest !--> 
The second model is the *Random forest model*. It seems promising because there is a large number of possible predictors, which gives the Random forest many opportunities for randomly selecting m variables to split on in the first step (i.e.decorrelation). This will lead to more differently looking bagged trees. Aggregating those leads then to more reduction in variance and thus a more robust outcome, increased stability and less overfitting. Moreover, using this model there is no need to make decisions regarding functional form, interaction, variable selection. In addition, the Random forest needs relatively little tuning. I again set the tuning parameters (number of trees, number of variables checked for a spit, and depth of trees (size)) by trying out several combinations via five-fold cross validation. In the cross-validation process I try out 3 different values (5, 7, 9) for the parameter 'mtry' (number of variables randomly sampled as candidates at each split). Usually the parameter roughly equals the square root of the number of variables. In this case the number of predictors is 26, so I set the starting value to be equal to its square root which is approximately  5.09. The next parameter,'.min.node.size' specifies the minimum size of terminal nodes (leaves) in the trees. Here I also chose 3 different values to try out, since smaller value allows the model to create more complex trees, potentially capturing more details in the data, but at the same time it may also lead to overfitting. A larger value results in simpler trees, which may generalize better to new data but might miss some patterns in the training data. The 'ntree' parameter, number of trees to grow in the forest, is left at its default value of 500. More trees can increase model performance, but it also requires more computational resources. Last, I set the splitting rule for building the trees to be variance reduction, since this splitting rule is less sensitive towards outliers and because I have a regression with a continuous target variable. The best performing model (according to the RMSE) uses: mtry = 9, splitrule = variance and min.node.size = 5 and has a RMSE of 679.6966  
```{r Random Forest, echo=FALSE, include=FALSE}
# Model 2: Random forest
# set tuning for random forest
tune_grid_RF <- expand.grid(
  .mtry = c(5, 7, 9),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

# simpler model for model A (1)
system.time({
  rf_model <- train(
    formula(paste0("price_n ~", paste0(predictors_ML, collapse = " + "))),
    data = listings_work,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid_RF,
    importance = "impurity"
  )})

rf_model

summary(rf_model)

# Show Model B rmse shown with all the combinations
rf_tuning_model <- rf_model$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

rf_tuning_model


# Turning parameter choice 
result_1 <- matrix(c(
  rf_model$finalModel$mtry,
  rf_model$finalModel$min.node.size
  ),
nrow=1, ncol=2,
dimnames = list(c("Model A"),
                c("Min vars","Min nodes"))
)
#After choosing the best model via cross-validation the algorithm reevaluates
```
<!-- Model 2: Boosting !--> 
The last model I consider to predict Airbnb pricing in Copenhagen is a *Boosting Algorithm*, or more specific gradient boosting using the gbm library. Similarly to the Random forest it is an ensemble method based on trees. However,it differs from Random forest by building sequential trees that depend on each other and in the end combines results from many weak and imperfect trees to make the final strong prediction. Ensemble approaches often leads to improved predictive performance compared to individual models since they capture diverse patterns and reduce overfitting. Overall its a flexible model that is capable of capturing complex, nonlinear relationships in the data. In the case of predicting Airbnb prices, there may be intricate interactions and dependencies between various features (e.g., neighborhood, amenities, property type) that boosting models can model effectively. In addition, Boosting models are less sensitive to outliers (i.e. here unusual or extreme listings) compared to other algorithms.
Similar tp before I begin by fine-tuning the model to the specific characteristics of the Airbnb data set using cross-validation. Compared to Random forest there is a greater emphasis on tuning with Boosting. The first parameter 'interaction.depth' guides the complexity of the tree. It represents the maximum depth of an individual tree. In this case, I try three different values (1, 5, and 10) since there is a trade-off between model complexity and interpretability. Lower values (e.g. 1) are chosen when the relationships in the data are suspected to be relatively simple. This leads to shallow trees, which can be computationally efficient and less prone to overfitting. Slightly larger values (e.g. 5) are appropriate for data sets with moderately complex relationships and balance between model complexity and computational efficiency. Higher values indicate (e.g. 10) that relationships in your data are highly intricate and nonlinear but are prone to over fit. The next parameter, 'n.trees' specifies the number of boosting iterations or trees, here I cross validate over a sequence from 200 to 700 with a step size of 50. The parameter depends on the trade-off between model performance and training time. The third parameter of interest is 'shrinkage' or the learning rate, this parameter controls the contribution of each tree to the final prediction. A lower value results in a more robust model but requires more trees. It is set to 0.1, which is a common starting value. Last, the parameter 'n.minobsinnode', the minimum number of training set samples required in a node to initiate a split during the construction of a tree, is set to 20. As it helps control the size of the tree nodes there is again a trade-off between model complexity and interpretability. Small values lead to complex trees and potential overfitting vice versa for large values. A value of 20 is commonly used in practice. The RMSE is used to select the optimal model using the smallest value. The final values used for the best model are n.trees = 200, interaction.depth = 10, shrinkage = 0.1 and n.minobsinnode = 20. It has a RMSE of 697.9551. 
```{r Boosting, echo=FALSE, include=FALSE, warning=F}
# Model 3: Boosting
# GBM  -------------------------------------------------------
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
                         n.trees = (4:10)*50, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20) # the minimum number of training set samples in a node to commence                                                     splitting
system.time({
  gbm_model <- train(formula(paste0("price_n ~", paste0(predictors_ML, collapse = " + "))),
                     data = listings_work,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)})
gbm_model
summary(gbm_model)
```

After being split into train and test data set during cross-validation the best model of LASSO, Random forest and Boosting gets reevaluated on the working data set. The results are shown in table 1. We can see in column 1 that out of the three models according to the RMSE (for the work data set) the best performing one is model 2 - the Random forest. It has a RMSE of: 679.6966 which is substantially lower than those of the other two models. Also in terms of other evaluation methods like the mean absolute error or the R-squared model 2 performs best. The second best is the Boosting algorithm and the last the LASSO. Moreover, from a practical perspective Random forest is the most convenient model as it is fully automatic, needs little tuning and works very fast compared to more complex Machine Learning algorithms like Boosting. Table 2 shows the results for reevaluating the three models on the hold-out data set. In terms of the performance ranking nothing changes and the Random forest model remains the winner. It is unexpected though that the RMSEs in the hold out set are lower for all three models, usually the opposite is the case since the model has to generalize to new, unseen data. The lower holdout RMSE might be due to the randomness involved in splitting the data into work and holdout sets which can lead to variability in performance metrics. Other explanations are model simplicity or outliers. 
Compared to the results from the London case study (slide 51) where the RMSE for the different models are all between 45 and 50, the RMSEs are substantially larger. Also the differences between the RMSE of the three models are quite big, compared to the results from the London case study. However, the absolute value of the RMSE does not have any meaning and the two data sets for London and Copenhagen differ in many regards. Furthermore, I believe the large size of the RMSE comes from the fact that in this data set the price is given in DKK instead of Euro, which is 7,46 times as much. It would be a good robustness check to transform the prices into EUR an recalculate the models.  
```{r,echo=FALSE, include=FALSE, warning=F}
# ---- compare these models

final_models <-
  list("LASSO (model w/ interactions)" = lasso_model,
       "Random forest" = rf_model,
       "GBM"  = gbm_model)

results_all <- resamples(final_models) %>% summary()
results_all

results_allDF <- as.data.frame(results_all$statistics)
results_allDF <- as.data.frame(t(results_allDF))

result_rmse <- imap(final_models, ~{
  mean(results_all$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Work set RMSE" = ".")
result_rmse
```

```{r,echo=FALSE, include=FALSE, warning=F}
# evaluate model on the holdout set -----------------------------

result_holdout <- map(final_models, ~{
  RMSE(predict(.x, newdata = listings_holdout), listings_holdout[["price_n"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout set RMSE" = ".")

result_holdout


##### mean price

data_holdout_w_prediction <- listings_holdout %>%
  mutate(predicted_price_lasso = predict(lasso_model, newdata = listings_holdout),
         predicted_price_rf = predict(rf_model, newdata = listings_holdout),
         predicted_price_gbm = predict(gbm_model, newdata = listings_holdout))

mean(data_holdout_w_prediction$predicted_price_lasso) ## needed for intro 
mean(data_holdout_w_prediction$predicted_price_rf) ## needed for intro #1166.516
mean(data_holdout_w_prediction$predicted_price_gbm) ## needed for intro , lowest

# heterogeneity evaluation based on subsamples????? Sl.31
```
**Model evaluation with mimicked live data**: Next, I evaluate the three models again but this time on the 'new' data set which has September 2023 as a reference date. I do this to further investigate the out of sample prediction performance of the three models. Using the September data set is a way to mimic live data which we cannot access in another way. The results in column 3 of table 1 show that again in terms of the performance ranking nothing changes and the Random forest model remains the model with the lowest RMSE of 728.6335. The LASSO model remains the worst performing model with a RMSE of 773.9909. As expected from theory the RMSEs of the 'unseen' data set are larger than the RMSE of the holdout data set and the working data set. Which leads to the conclusion that the predictive power of the models decreases when using 'live' data. The problem of the lower holdout RMSE seems not to prevail here if a different 'unseen' data set is used. 
```{r,echo=FALSE, include=FALSE, warning=F}
# evaluate preferred model on september 23 data set -----------------------------

result_SEP <- map(final_models, ~{
  RMSE(predict(.x, newdata = listings_clean_SEP), listings_clean_SEP[["price_n"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("September RMSE" = ".")

result_SEP

```

```{r,echo=FALSE}
# Combine data frames column-wise
combined_df <- cbind(result_rmse, result_holdout, result_SEP)

# Print the combined data frame
#print(combined_df)

kable(combined_df,digits=2,caption = "RMSE for all data sets") 

```
**Model evaluation and visualization Random Forest:**
Since Machine Learning methods are often seen as "black-box" models, and we often do not really know how an actual prediction is created I want to further investigate why the Random forest model was the best performing model in all scenarios discussed above. One way to do this is by looking at the featuresâ€™ contribution to predicted values on average i.e. variable importance using variance importance plots and Shapley values. Understanding what factors influence Airbnb prices the most can help users and hosts to make informed decisions. First, looking at graph x which displays the 10 most important features (in percentage terms) of the Random forest we can see that the variables 'accommodates','beds'& 'n_days_since' contribute the most. Together these three variables explain about 30% of the prediction performance. The variables 'review_score_rating' and 'neigborhood_cleansedIndre By' also appear to be influential. For all other variables the importance lays below 6% and diminishes rather quickly. The results are similar to the London case study, where also 'accommodates' was the most important variable. Grouping the factor variables together and recalculating the variable importance indicates no change in the importance ordering (see Appendix graph x). In addition, graph x in the appendix displays the importance of all features in the model above a cutoff (for readability). Here again we see the typical shape that three to four variables are quite important in explaining the prediction outcome and the others contribute relatively little. Turning to the Shaeply values, a local model agnostic approach, which calculates marginal importance of the variables. In detail, the Shapley plot x puts the baseline value (= sample mean of price_n: 1494.67) on the horizontal axis and shows how each feature adds (or subtracts) for a given observation. It illustrates that 'neigborhood_cleansedIndre By' has the highest relative importance by adding a value of +277.42 to the sample mean. This outcome seems reasonable since the neighborhood Indre By in Copenhagen, is very central, close to the harbor and close to most of the touristic hot-spots. Further, the graph highlights that the features 'accommodates','beds'& 'n_days_since are again important. However, now we can see in much more detail how each of their contributions distributes to the prediction. For example if an Airbnb can accommodate 3 people, the sample mean of price_n drops by -70.64. If the Airbnb has two beds, the first review is older than 10 year or the review score rating is equal to 5 there is a positive effect on the sample mean of the Airbnb price. An even deeper analysis between each value of a predictor and the predicted values gives us the Partial dependence plot (see Appendix, graph for the most important feature 'accommodates'). Even more insights could be obtained from an analysis across sub samples. This is however neglected here due to space & time constraints.
```{r,echo=FALSE, include=FALSE, warning=F}
# use RF model since performed the best!
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_var_imp <- importance(rf_model$finalModel)/1000
rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
```

```{r,echo=FALSE, include=FALSE, warning=F}
##############################
# 1) full varimp plot, above a cutoff
##############################

cutoff = 600

rf_model_var_imp_plot <- ggplot(rf_model_var_imp_df[rf_model_var_imp_df$imp > cutoff,],
                                aes(x = reorder(varname, imp), y = imp_percentage)) +
  geom_point(color = color[1], size = 1.5) +
  geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +  # Change to theme_minimal or another theme of your choice
  theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines only
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6))

#rf_model_var_imp_plot
```
```{r,echo=FALSE, include=FALSE, warning=F}
##############################
# 2) full varimp plot, top 10 only
##############################

rf_model_var_imp_plot_10 <- ggplot(rf_model_var_imp_df[1:10,], aes(x = reorder(varname, imp), y = imp_percentage)) +
  geom_point(color = color[1], size = 1) +
  geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +  # Change to theme_minimal or another theme of your choice
  theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines only
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6))

#rf_model_var_imp_plot_10
```


```{r, figures-VARIMP, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
rf_model_var_imp_plot_10
```

```{r,echo=FALSE, include=FALSE, warning=F}
##############################
# 2) varimp plot grouped to (8)
##############################
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model$finalModel$xNames
f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)

groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
               f_property_type = f_property_type_varnames,
               f_room_type = f_room_type_varnames,
               bathroom = "bathroom",
               n_days_since = "n_days_since",
               accommodates = "accommodates",
               beds = "beds")

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                            imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_grouped_plot <- ggplot(rf_model_var_imp_grouped_df, aes(x = reorder(varname, imp), y = imp_percentage)) +
  geom_point(color = color[1], size = 1) +
  geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal() +  # Change to theme_minimal or another theme of your choice
  theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines only
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6))
#rf_model_var_imp_grouped_plot
```


```{r,echo=FALSE, include=FALSE, warning=F}
## SHAP VALUES ##
# devtools::install_github('ModelOriented/treeshap')

#define one-hot encoding function
dummy <- dummyVars(" ~  accommodates + beds + f_property_type + f_room_type + n_days_since + flag_days_since + neighbourhood_cleansed + bathrooms + f_number_of_reviews + review_scores_rating + flag_review_scores_rating + a_Parking + a_LongTermStay + a_Kitchen + a_Childfriendly + a_Laundry + a_OutdoorArea + a_Heating + a_Toiletry + a_TeaCoffee + a_Internet + a_Entertainment + a_Security + a_Selfcheckin + a_HomeOffice + a_Petfriendly", data=listings_holdout, fullRank=T, sep = NULL)

#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=listings_holdout))

# replace "." character to " " to match model object names
names(data_holdout_ohe) <- gsub(x = names(data_holdout_ohe),
                                pattern = "\\.", 
                                replacement = " ")  

# unify model for treeshap
rf_model_unified <- ranger.unify(rf_model$finalModel, data_holdout_ohe)
# when: Error in set_reference_dataset(ret, as.data.frame(data)) : 
# Dataset does not contain all features occurring in the model.

#model_variable_names <- rf_model$coefnames
#new_data_variable_names <- colnames(data_holdout_ohe)

#missing_variables <- setdiff(rf_model$coefnames, new_data_variable_names)
#if (length(missing_variables) > 0) {
 # stop(paste("Error: Missing variables in new data:", toString(missing_variables)))
#}
# if Error message (add variables from Error here manually)
# Identify the indices of the columns you want to change
indices_to_change <- which(colnames(data_holdout_ohe) %in% c("neighbourhood_cleansedBrnshj Husum", "neighbourhood_cleansedVesterbro Kongens Enghave"))

# Create a vector of new column names for the selected columns
new_names <- c("neighbourhood_cleansedBrnshj-Husum", "neighbourhood_cleansedVesterbro-Kongens Enghave")

# Change the column names only for the selected columns
names(data_holdout_ohe)[indices_to_change] <- new_names

## go back to unify model for treeshap

#rf_model_unified <- ranger.unify(rf_model$finalModel, data_holdout_ohe)

#treeshap_res <- treeshap(rf_model_unified, data_holdout_ohe[1:500, ]) # delete # for calculation but excluded here for Rmd knitting 

#treeshap_res <- plot_contribution(treeshap_res, obs = 12)

#treeshap_featureIMP <- plot_feature_importance(treeshap_res, max_vars = 10)

#treeshap_inter <- treeshap(rf_model_unified, data_holdout_ohe[1:100, ], interactions = T)
#summary(treeshap_res)
```

```{r, figures-VARIMP, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
treeshap_res
treeshap_featureIMP
```



```{r,echo=FALSE, include=FALSE, warning=F}
#########################################################################################
# Partial Dependence Plots -------------------------------------------------------
#########################################################################################
pdp_n_acc <- pdp::partial(rf_model, pred.var = "accommodates", pred.grid = distinct_(listings_holdout, "accommodates"), train = listings_holdout)
pdp_n_acc_plot <- pdp_n_acc %>%
  autoplot( ) +
  geom_point(color=color[1], size=2) +
  geom_line(color=color[1], size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
  theme_bg()
#pdp_n_acc_plot

```

<!-- References 
ChatGPT, Filter Excluded Property Types, November 21, 2023.
Retrieved from: https://chat.openai.com/share/c7ed8e6b-ffa8-46f0-97e4-672bff890d60

BÃ©kÃ©s,G., KÃ©zdi,G.(2021).R, Python and Stata code for Data Analysis for Business, Economics, and Policy, ch14-airbnb-reg & ch16-airbnb-random-forest  , GitHub repository, https://github.com/gabors-data-analysis/da_case_studies/tree/master/ch14-airbnb-reg
https://github.com/gabors-data-analysis/da_case_studies/tree/master/ch16-airbnb-random-forest

BÃ©kÃ©s, G., & KÃ©zdi, G. (2021). Data analysis for business, economics, and policy. Cambridge University Press, chapter 14&16.!--> 

## Appendix

```{r,echo=FALSE, include=TRUE, warning=FALSE}
# data summary
#kable(skimr::skim(listings_clean))
```


```{r,echo=FALSE}
kable(results_allDF,digits = 2,booktabs=TRUE, linesep = "",caption = "Metrics all")
```

```{r, figures-side, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
par(mar = c(4, 4, .1, .1))
p1_lnprice
p2_price
```


```{r, figures-side2, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
par(mar = c(4, 4, .1, .1))
# Feature engineering: functional form 
#p3_accommodates # scatter
#p4_nreview # bar
#p5_lnreview# bar
#p6_beds# bar
#p7_bathrooms# bar
p8_lndays
p9_reviewsr
```


```{r, figures-side3, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
par(mar = c(4, 4, .1, .1))
# Feature engineering: interactions 
p10_roomtypebox
p11_accomdatesbox
```

```{r, figures-side4, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
par(mar = c(4, 4, .1, .1))
# Feature engineering: interactions 
rf_model_var_imp_plot
rf_model_var_imp_grouped_plot

```
```{r, figures-side5, fig.show="hold", out.width="50%", echo =FALSE, warning=FALSE}
#par(mar = c(4, 4, .1, .1))
# PDP accomodates
pdp_n_acc_plot
```




################################################################################
<!-- Additional OLS model for check-up (not included in report) !--> 
```{r, include=FALSE, echo= FALSE}

system.time({
ols_model <- train(
  formula(paste0("price_n ~", paste0(predictors_LASSO, collapse = " + "))),
  data = listings_work,
  method = "lm",
  trControl = train_control
)
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

summary(ols_model)

```

